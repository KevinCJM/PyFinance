# -*- coding: utf-8 -*-
"""
A01_preprocess.py

ä¸€ä¸ªâ€œä¸“é—¨ç”¨äºæ–°é—»é¢„å¤„ç†â€çš„ç‹¬ç«‹è„šæœ¬ï¼š
- ä»…åšæ–‡æœ¬è§„èŒƒåŒ–ä¸æ¸…æ´—ï¼ˆä¸åšâ€œç›¸åŒæ–°é—»â€è¯†åˆ«ï¼‰ã€‚
- é¢å‘ä¸­æ–‡æ–°é—»ï¼Œä½¿ç”¨å‘é‡åŒ– pandas å­—ç¬¦ä¸²æ“ä½œä¸é¢„ç¼–è¯‘æ­£åˆ™ï¼Œå°½é‡é¿å…ä½æ•ˆå¾ªç¯ä¸é“¾å¼èµ‹å€¼è­¦å‘Šã€‚
- è¾“å‡ºè§„èŒƒåŒ–åˆ—ï¼štitle_norm, text_norm, doc_normï¼›å¹¶å‰”é™¤ç©ºæ–‡æœ¬è¡Œã€‚

ç”¨æ³•ç¤ºä¾‹ï¼š
    python A01_preprocess.py \
        --input test_news.parquet \
        --out-parquet test_news_clean.parquet \
        --out-excel test_news_clean.xlsx

å¯é€‰å¼€å…³ï¼š
    --drop-dup-title    æ¸…æ´—åæŒ‰ title_norm å»é‡ï¼ˆä»…é¢„å¤„ç†é˜¶æ®µçš„â€œæ˜¾å¼é‡å¤â€ï¼‰
    --drop-dup-text     æ¸…æ´—åæŒ‰ text_norm å»é‡
    --keep-original     è¾“å‡ºæ—¶æºå¸¦åŸå§‹ title/text åˆ—ï¼ˆé»˜è®¤ä¹Ÿä¼šä¿ç•™ï¼‰

æ³¨æ„ï¼š
- è¿™é‡Œä¸åšâ€œç›¸åŒæ–°é—»/ç›¸ä¼¼æ–°é—»â€çš„èšç±»æˆ–é˜ˆå€¼åˆ¤æ–­ï¼›é‚£éƒ¨åˆ†åœ¨ä¸‹ä¸€æ­¥å•ç‹¬è„šæœ¬å®ç°ã€‚
- ä¾èµ–ï¼špandas, numpyï¼ˆä¸ reï¼‰ã€‚
"""

import os
import sys
import re
import argparse
from typing import Iterable, Optional, Tuple, Dict

import numpy as np
import pandas as pd

# ---------- 1) é¢„ç¼–è¯‘æ­£åˆ™ä¸æ›¿æ¢è¡¨ ----------

# ç»Ÿä¸€ç©ºç™½
RE_SPACES = re.compile(r'\s+')

# HTML æ®‹ç•™
RE_HTML_ENT = re.compile(r'&lt;/?div&gt;|&nbsp;')

# $ åé¢ä¸æ˜¯æ•°å­—çš„ $
RE_DOLLAR_NONNUM = re.compile(r'\$(?!\d)')

# å¼€å¤´åƒ â€œITä¹‹å®¶6æœˆ3æ—¥æ¶ˆæ¯,â€ çš„å¤´
RE_IT_HOUSE = re.compile(r'^ITä¹‹å®¶\d{1,2}æœˆ\d{1,2}æ—¥æ¶ˆæ¯,')

# æ‹¬å·å†…å™ªå£°ï¼ˆæ¥æºã€è®°è€…ã€åŸæ ‡é¢˜ç­‰ï¼‰
RE_PAREN_NOISE = re.compile(
    r'\([^)]*(è®°è€…|æ¥æº|ç‰¹çº¦é€šè®¯å‘˜|é€šè®¯å‘˜|å›¾æº|æ–‡/|ä»¥ä¸‹ä¿¡æ¯ä»|ä»¥ä¸‹å†…å®¹ä»|åŸæ ‡é¢˜|æ‘˜è¦|ç§‘æŠ€æ¶ˆæ¯|æœ¬æŠ¥è®¯|Vè§‚è´¢æŠ¥|ç¼–è€…æŒ‰|è´¢åç¤¾è®¯|è´¢ç»æ—¥å†|ç¬¬ä¸€è´¢ç»|ç”µé³—è´¢ç»|æ³•æ²»å‘¨æœ«|ç§‘åˆ›æ¿æ—¥æŠ¥|é‡‘è¯ç ”|æ£€å¯Ÿæ—¥æŠ¥|åŸºé‡‘ç»ç†æŠ•èµ„ç¬”è®°|æ¯æ—¥ç»æµæ–°é—»)[^)]*\)'
)

# â€œæœ¬æŠ¥â€¦è®°è€…â€¦æŠ¥é“â€æ¨¡æ¿ï¼ˆå«å¯é€‰æ‹¬å·/åœ°ç‚¹ï¼‰
RE_BAODAO = re.compile(
    r'æœ¬æŠ¥(?:è®¯)?'  # å¯é€‰â€œæœ¬æŠ¥è®¯â€
    r'(?:ï¼ˆ[^ï¼‰]*ï¼‰|\([^)]*\)|ã€[^ã€‘]*ã€‘|\[[^\]]*\])?'  # å¯é€‰æ‹¬å·å—ï¼ˆå…¨/åŠè§’/æ–¹æ‹¬/ä¹¦åå·ï¼‰
    r'[^,.;:ï¼Œã€‚ï¼›ï¼š]*?è®°è€…'  # åˆ°â€œè®°è€…â€ä¸ºæ­¢ï¼ˆä¸è·¨å¥ï¼‰
    r'[^,.;:ï¼Œã€‚ï¼›ï¼š]*?æŠ¥é“'  # åˆ°â€œæŠ¥é“â€ä¸ºæ­¢ï¼ˆä¸è·¨å¥ï¼‰'
)

# å‹ç¼©è¿ç»­çš„å¥ç‚¹
RE_MULTI_DOTS = re.compile(r'\.{2,}')

# å•å­—ç¬¦æ˜ å°„ï¼ˆtranslate æ›´å¿«ï¼‰ï¼›åŒ…å«åŠè§’åŒ–/ç»Ÿä¸€æ ‡ç‚¹/å»è£…é¥°ç¬¦
SINGLE_CHAR_MAP = {
    'ã€': ',', 'ï¼Œ': ',', 'ã€‚': '.', 'ï¼': '!', 'ï¼Ÿ': '?', 'ï¼›': ';', 'ï¼š': ':',
    'ï¼ˆ': '(', 'ï¼‰': ')', 'ã€': '(', 'ã€‘': ')', 'ã€Š': '(', 'ã€‹': ')',
    'â€œ': '"', 'â€': '"', 'â€˜': '"', 'â€™': '"', 'ã€': '"', 'ã€': '"',
    'ãƒ»': '-', 'ï½': '-', 'â€”': '-', 'â€¦': '.',  # å¤šä¸ªç‚¹åç»­ç»Ÿä¸€å‹ç¼©
    'Â·': '', 'â—': '', 'â– ': '', 'â–²': '', 'â–¼': '', 'â–': '', 'â–': '',
    'â–Œ': '', 'â–': '', 'â–¡': '', 'â–³': '', 'â–½': '', 'â–º': '', 'â–¶': '',
    'â—€': '', 'â—†': '', 'â—‡': '', 'â˜…': '', 'â˜†': '', 'â€»': '', 'â—': '',
    'ã€“': '', '#': '', '*': '', 'ï¼Š': '', 'Â§': '', 'Â©': '', 'Â®': '',
    '[': '(', ']': ')', '{': '(', '}': ')', '<': '(', '>': ')'
}
TRANS_TABLE = str.maketrans(SINGLE_CHAR_MAP)

# ç«™ç‚¹/å£å·ç±»çŸ­è¯­ç»Ÿä¸€ï¼ˆå¯æŒ‰éœ€è¡¥å……ï¼‰
PHRASE_MAP = {
    'é˜¿é‡Œå·´å·´é›†å›¢': 'é˜¿é‡Œ', 'é˜¿é‡Œå·´å·´': 'é˜¿é‡Œ',
    'åŒ—äº¬å•†æŠ¥è®¯': '', 'åŒ—äº¬å•†æŠ¥': '',
    'cninfo.com.cn': 'cninfo', 'cninfo.com': 'cninfo',
    'DoNews.com': 'DoNews', 'DoNews.cn': 'DoNews',
    'Windæ•°æ®æ˜¾ç¤º,': '', 'Windèµ„è®¯,': '', 'Windèµ„è®¯': '',
    'YYç‚¹è¯„:': '', 'YYç‚¹è¯„': '',
    'ğŸ‘‡ç‚¹å‡»ä¸‹æ–¹ğŸ‘‡ç»„å›¢é›†å¡æŠ½çº¢åŒ…,çœåˆ°å°±æ˜¯èµšåˆ°': '',
    'irm.cn': 'irm', 'eå…¬å¸è®¯,': '',
    '????': '', '???': ''
}
if PHRASE_MAP:
    # æ„é€ ä¸€æ¬¡æ€§æ­£åˆ™ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œé¿å…å­ä¸²å¹²æ‰°ï¼‰
    _phr_keys = sorted(PHRASE_MAP.keys(), key=len, reverse=True)
    RE_PHRASES = re.compile('|'.join(map(re.escape, _phr_keys)))
else:
    RE_PHRASES = None


# ---------- åŸºç¡€å·¥å…· ----------

def _normalize_series(s: pd.Series) -> pd.Series:
    """å‘é‡åŒ–è§„èŒƒåŒ–ï¼šç»Ÿä¸€ç©ºç™½/æ ‡ç‚¹ï¼Œç§»é™¤å¸¸è§å™ªå£°æ¨¡æ¿ã€‚"""
    s = s.fillna('')
    s = s.str.replace(RE_SPACES, '', regex=True)
    s = s.str.replace(RE_HTML_ENT, '', regex=True)
    s = s.str.translate(TRANS_TABLE)
    if RE_PHRASES is not None:
        s = s.str.replace(RE_PHRASES, lambda m: PHRASE_MAP.get(m.group(0), ''), regex=True)
    s = s.str.replace(RE_DOLLAR_NONNUM, '', regex=True)
    s = s.str.replace(RE_PAREN_NOISE, '', regex=True)
    s = s.str.replace(RE_BAODAO, '', regex=True)
    s = s.str.replace(RE_IT_HOUSE, '', regex=True)
    s = s.str.replace(RE_MULTI_DOTS, '.', regex=True)
    return s


def preprocess_dataframe(df: pd.DataFrame,
                         drop_dup_title: bool = False,
                         drop_dup_text: bool = False,
                         keep_original: bool = True) -> pd.DataFrame:
    """
    å¯¹è¾“å…¥çš„ DataFrame è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†ï¼Œè¦æ±‚è‡³å°‘åŒ…å« ['title', 'text'] åˆ—ã€‚

    é¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬ï¼š
    - ä¸¥æ ¼å»é™¤ç©ºå€¼ï¼ˆå°†ç©ºå­—ç¬¦ä¸²è§†ä½œ NaNï¼‰ï¼›
    - å¯¹ title å’Œ text å­—æ®µè¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼Œç”Ÿæˆ title_norm å’Œ text_normï¼›
    - åˆæˆ doc_norm å­—æ®µï¼štitle_norm + 'ã€‚' + text_normï¼›
    - å¯é€‰åœ°æ ¹æ® title_norm æˆ– text_norm å»é™¤é‡å¤è¡Œï¼›
    - è¿”å›é¢„å¤„ç†åçš„ DataFrameï¼Œä¸è¿›è¡Œèšç±»ç­‰åç»­å¤„ç†ã€‚

    å‚æ•°ï¼š
        df (pd.DataFrame): è¾“å…¥çš„ DataFrameï¼Œå¿…é¡»åŒ…å« 'title' å’Œ 'text' åˆ—ã€‚
        drop_dup_title (bool): æ˜¯å¦æ ¹æ® title_norm å»é‡ï¼Œé»˜è®¤ä¸º Falseã€‚
        drop_dup_text (bool): æ˜¯å¦æ ¹æ® text_norm å»é‡ï¼Œé»˜è®¤ä¸º Falseã€‚
        keep_original (bool): æ˜¯å¦ä¿ç•™åŸå§‹çš„ title å’Œ text åˆ—ï¼Œé»˜è®¤ä¸º Trueã€‚

    è¿”å›ï¼š
        pd.DataFrame: é¢„å¤„ç†åçš„ DataFrameï¼ŒåŒ…å« title_normã€text_norm å’Œ doc_norm åˆ—ï¼Œ
                      è‹¥ keep_original ä¸º Trueï¼Œåˆ™è¿˜ä¿ç•™åŸå§‹çš„ title å’Œ text åˆ—ã€‚
    """
    if not {'title', 'text'}.issubset(df.columns):
        raise ValueError("è¾“å…¥ DataFrame å¿…é¡»åŒ…å«åˆ—ï¼š['title','text']")

    out_cols = ['title', 'text'] if keep_original else []
    # ä¸¥æ ¼å»ç©ºï¼ˆç©ºä¸²ç­‰ä»· NaNï¼‰
    df = df[['title', 'text']].copy()
    df['title'] = df['title'].replace('', np.nan)
    df['text'] = df['text'].replace('', np.nan)
    df = df.dropna(subset=['title', 'text']).reset_index(drop=True)

    # è§„èŒƒåŒ– title å’Œ text åˆ—ï¼Œç”Ÿæˆå¯¹åº”çš„ *_norm åˆ—
    title_norm = _normalize_series(df['title'])
    text_norm = _normalize_series(df['text'])

    # æ‹¼æ¥ title_norm å’Œ text_norm å¾—åˆ° doc_norm
    doc_norm = title_norm.str.cat(text_norm, sep='ã€‚', na_rep='').str.strip()

    # ç»„è£…è¾“å‡º DataFrame
    res = pd.DataFrame({
        **({'title': df['title']} if keep_original else {}),
        **({'text': df['text']} if keep_original else {}),
        'title_norm': title_norm,
        'text_norm': text_norm,
        'doc_norm': doc_norm
    })

    # åˆ é™¤ doc_norm ä¸ºç©ºçš„è¡Œ
    res = res[res['doc_norm'].str.len() > 0].reset_index(drop=True)

    # å¯é€‰ï¼šæŒ‰è§„èŒƒåŒ–åçš„ title æˆ– text å»é‡
    if drop_dup_title:
        res = res.loc[~res['title_norm'].duplicated()].reset_index(drop=True)
    if drop_dup_text:
        res = res.loc[~res['text_norm'].duplicated()].reset_index(drop=True)

    return res


# ---------- IO ----------

def read_any(input_path: str) -> pd.DataFrame:
    """æ ¹æ®æ‰©å±•åè‡ªåŠ¨è¯»å– parquet/csv/json/jsonlã€‚"""
    ext = os.path.splitext(input_path)[1].lower()
    if ext in ('.parquet', '.pq'):
        return pd.read_parquet(input_path)
    if ext in ('.csv',):
        return pd.read_csv(input_path)
    if ext in ('.jsonl', '.jsonl.gz'):
        return pd.read_json(input_path, lines=True)
    if ext in ('.json',):
        return pd.read_json(input_path)
    # é»˜è®¤å°è¯• parquetï¼Œå†é€€å› csv
    try:
        return pd.read_parquet(input_path)
    except Exception:
        return pd.read_csv(input_path)


def write_outputs(df: pd.DataFrame,
                  out_parquet: Optional[str],
                  out_excel: Optional[str]) -> None:
    if out_parquet:
        df.to_parquet(out_parquet, index=False)
        print(f"[INFO] ä¿å­˜åˆ° {out_parquet} å®Œæˆ")
    if out_excel:
        df.to_excel(out_excel, index=False)
        print(f"[INFO] ä¿å­˜åˆ° {out_excel} å®Œæˆ")


def main(drop_dup_title=True, drop_dup_text=True, keep_original=True,
         input_pq='test_news.parquet', out_pq='test_news_clean.parquet', out_excel='test_news_clean.xlsx'):
    print(f"[INFO] è¯»å–ï¼š{input_pq}")
    df = read_any(input_pq)

    # éªŒè¯è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«å¿…éœ€çš„åˆ—ï¼Œå¹¶å°è¯•è‡ªåŠ¨é€‚é…å¤§å°å†™ä¸åŒçš„åˆ—å
    if not {'title', 'text'}.issubset(df.columns):
        # å…è®¸ä¸åŒå¤§å°å†™/è¯­è¨€ç¯å¢ƒçš„åˆ—åï¼Œå°è¯•è‡ªåŠ¨é€‚é…
        lower_map = {c.lower(): c for c in df.columns}
        if 'title' in lower_map and 'text' in lower_map:
            df = df.rename(columns={lower_map['title']: 'title', lower_map['text']: 'text'})
        else:
            raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»åŒ…å«åˆ— 'title' ä¸ 'text'ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰ã€‚")

    print(f"[INFO] åŸå§‹æ¡æ•°ï¼š{len(df)}")

    # æ‰§è¡Œæ•°æ®é¢„å¤„ç†ï¼ŒåŒ…æ‹¬å»é‡ç­‰æ“ä½œ
    res = preprocess_dataframe(
        df,
        drop_dup_title=drop_dup_title,
        drop_dup_text=drop_dup_text,
        keep_original=keep_original  # é»˜è®¤ä¿ç•™åŸå§‹åˆ—ï¼Œä¾¿äºå®¡è®¡
    )

    print(f"[INFO] é¢„å¤„ç†å®Œæˆï¼Œå‰©ä½™æ¡æ•°ï¼š{len(res)}")

    write_outputs(res, out_pq, out_excel)
    print(f"[INFO] å†™å‡ºå®Œæˆã€‚")


if __name__ == '__main__':
    ''' æœ€ç»ˆç»“æœæœ‰4åˆ—: title, text, title_norm, text_norm, doc_norm
    åˆ†åˆ«è¡¨ç¤º: æ ‡é¢˜, æ­£æ–‡, è§„èŒƒåŒ–æ ‡é¢˜, è§„èŒƒåŒ–æ­£æ–‡, åˆæˆçš„æ–‡æ¡£æ–‡æœ¬
    '''
    main(drop_dup_title=True, drop_dup_text=True, keep_original=True,
         input_pq='test_news.parquet',
         out_pq='test_news_clean.parquet', out_excel='test_news_clean.xlsx'
         )
