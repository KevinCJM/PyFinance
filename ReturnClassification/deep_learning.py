# -*- encoding: utf-8 -*-
"""
@File: test.py.py
@Modify Time: 2025/4/21 19:35
@Author: Kevin-Chen
@Descriptions:
"""
import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report
from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Flatten, Add, Multiply, Reshape
from tensorflow.keras.layers import Dropout, BatchNormalization, Activation, Reshape, Lambda
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from sklearn.metrics import f1_score
import tensorflow as tf


# è‡ªå®šä¹‰FMå±‚ï¼ˆä½¿ç”¨Lambdaå°è£…tfæ“ä½œï¼‰
def fm_cross_layer(embed):
    """
    å®ç°FMï¼ˆFactorization Machineï¼‰æ¨¡å‹çš„äº¤å‰å±‚è®¡ç®—ã€‚

    å‚æ•°:
    embed: è¾“å…¥çš„ç‰¹å¾åµŒå…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, num_features, embedding_size)ã€‚

    è¿”å›:
    fm_output: è®¡ç®—å¾—åˆ°çš„FMäº¤å‰é¡¹è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, embedding_size)ã€‚
    """
    # è®¡ç®—æ‰€æœ‰ç‰¹å¾åµŒå…¥å‘é‡çš„å’Œ
    summed_features_emb = tf.reduce_sum(embed, axis=1)  # âˆ‘v_i
    # è®¡ç®—ä¸Šè¿°å’Œå‘é‡çš„å¹³æ–¹
    summed_features_emb_square = tf.square(summed_features_emb)  # (âˆ‘v_i)^2

    # è®¡ç®—æ¯ä¸ªç‰¹å¾åµŒå…¥å‘é‡çš„å¹³æ–¹
    squared_features_emb = tf.square(embed)  # v_i^2
    # è®¡ç®—ä¸Šè¿°å¹³æ–¹å‘é‡çš„å’Œ
    squared_sum_features_emb = tf.reduce_sum(squared_features_emb, axis=1)  # âˆ‘v_i^2

    # æ ¹æ®FMæ¨¡å‹çš„äº¤å‰é¡¹å…¬å¼ï¼Œè®¡ç®—è¾“å‡º
    fm_output = 0.5 * (summed_features_emb_square - squared_sum_features_emb)
    return fm_output


# æ„å»º DeepFM æ¨¡å‹
def build_deep_fm(input_dim,
                  embed_dim=8,
                  dnn_units=None,
                  dropout_rate=0.5,
                  activation='relu',
                  use_batch_norm=False,
                  learning_rate=0.001
                  ):
    """
    æ„å»ºDeepFMæ¨¡å‹ã€‚

    DeepFMæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ ç‰¹å¾é€‰æ‹©æ¨¡å‹ï¼Œå®ƒç»“åˆäº†å› å­åˆ†è§£æœºï¼ˆFMï¼‰å’Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œ
    ç”¨äºåŒæ—¶æ•æ‰çº¿æ€§å’Œéçº¿æ€§çš„ç‰¹å¾äº¤äº’ã€‚

    å‚æ•°:
    - input_dim (int): è¾“å…¥ç‰¹å¾çš„ç»´åº¦ã€‚
    - embed_dim (int): ç‰¹å¾åµŒå…¥çš„ç»´åº¦ã€‚é»˜è®¤ä¸º8ã€‚
    - dnn_units (list of int): DNNéƒ¨åˆ†æ¯å±‚çš„å•å…ƒæ•°ã€‚é»˜è®¤ä¸º[128, 64, 32]ã€‚
    - dropout_rate (float): Dropoutå±‚çš„æ¯”ä¾‹ã€‚é»˜è®¤ä¸º0.5ã€‚
    - activation (str): DNNéƒ¨åˆ†ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚é»˜è®¤ä¸º'relu'ã€‚
    - use_batch_norm (bool): æ˜¯å¦åœ¨DNNéƒ¨åˆ†ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ã€‚é»˜è®¤ä¸ºFalseã€‚
    - learning_rate (float): ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ã€‚é»˜è®¤ä¸º0.001ã€‚

    è¿”å›:
    - tf.keras.Model: æ„å»ºçš„DeepFMæ¨¡å‹ã€‚
    """
    # è¾“å…¥å±‚
    inputs = Input(shape=(input_dim,))

    # å¦‚æœæœªæŒ‡å®šDNNå±‚çš„å•å…ƒæ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if dnn_units is None:
        dnn_units = [128, 64, 32]

    # Embeddingå±‚ ğŸ‘‰ æŠŠæ¯ä¸ªç¦»æ•£ç‰¹å¾è½¬æ¢ä¸ºä¸€ä¸ªè¿ç»­çš„ä½ç»´å‘é‡è¡¨ç¤º
    embed = Dense(input_dim * embed_dim, activation='linear')(inputs)
    embed = Reshape((input_dim, embed_dim))(embed)

    # FMäº¤å‰é¡¹éƒ¨åˆ† ğŸ‘‰ æ˜¾å¼å»ºæ¨¡ä½é˜¶ç‰¹å¾äº¤å‰å…³ç³»
    fm_output = Lambda(fm_cross_layer)(embed)

    # DNNéƒ¨åˆ† ğŸ‘‰ å­¦ä¹ é«˜é˜¶ç»„åˆå’Œéçº¿æ€§å˜æ¢
    x = Flatten()(embed)
    for units in dnn_units:
        x = Dense(units)(x)
        # å¦‚æœä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–
        if use_batch_norm:
            x = BatchNormalization()(x)
        x = Activation(activation)(x)
        # å¦‚æœDropoutç‡å¤§äº0
        if dropout_rate > 0:
            x = Dropout(dropout_rate)(x)

    # åˆå¹¶FMå’ŒDNNéƒ¨åˆ†çš„è¾“å‡º
    concat = Concatenate()([fm_output, x])
    # è¾“å‡ºå±‚
    output = Dense(1, activation='sigmoid')(concat)

    # æ„å»ºæ¨¡å‹
    if learning_rate is None:  # å¦‚æœæ²¡æœ‰æä¾›å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„åŠ¨æ€å­¦ä¹ ç‡
        lr_schedule = ExponentialDecay(
            initial_learning_rate=0.0001,
            decay_steps=10000,
            decay_rate=0.1,
            staircase=True
        )
        optimizer = Adam(learning_rate=lr_schedule)
    else:  # å¦‚æœæä¾›äº†å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„å­¦ä¹ ç‡
        optimizer = Adam(learning_rate=learning_rate)

    # å®šä¹‰æ¨¡å‹å¹¶ç¼–è¯‘
    model = Model(inputs=inputs, outputs=output)
    model.compile(
        loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )
    return model


# DeepFM æ¨¡å‹è‡ªåŠ¨è°ƒå‚
def objective_deep_fm(trial,
                      validation_mode='kfold',
                      n_splits=5
                      ):
    """
    Optunaè¯•éªŒçš„ç›®æ ‡å‡½æ•°ï¼Œç”¨äºä¼˜åŒ–DeepFMæ¨¡å‹çš„è¶…å‚æ•°ã€‚

    å‚æ•°:
    - trial: Optunaçš„è¯•éªŒå¯¹è±¡ã€‚
    - validation_mode: 'split' æˆ– 'kfold'ã€‚
    - n_splits: KFoldçš„æŠ˜æ•°ï¼Œé»˜è®¤5æŠ˜ã€‚

    è¿”å›:
    - å¹³å‡F1åˆ†æ•°ï¼Œä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚
    """

    # è¶…å‚æ•°ç©ºé—´å®šä¹‰
    embed_dim = trial.suggest_categorical('embed_dim', [4, 8, 16, 32])
    dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.6)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True)
    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])
    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])
    dnn_units = trial.suggest_categorical("dnn_units", [
        [128, 64],
        [128, 64, 32],
        [128, 64, 32, 16],
        [64, 32],
        [64, 32, 16],
        [64, 32, 16, 8],
        [32, 16],
        [32, 16, 8],
    ])

    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)

    if validation_mode == 'split':
        x_subtrain, x_val, y_subtrain, y_val = train_test_split(
            x_train, y_train, test_size=0.1, shuffle=False
        )

        model = build_deep_fm(
            input_dim=x_train.shape[1],
            embed_dim=embed_dim,
            dnn_units=dnn_units,
            dropout_rate=dropout_rate,
            activation=activation,
            use_batch_norm=use_batch_norm,
            learning_rate=learning_rate
        )

        model.fit(
            x_subtrain, y_subtrain,
            validation_data=(x_val, y_val),
            epochs=30,
            batch_size=32,
            verbose=0,
            callbacks=[early_stop]
        )

        y_val_pred = (model.predict(x_val) > 0.5).astype(int)
        return f1_score(y_val, y_val_pred)

    elif validation_mode == 'kfold':
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
        f1_scores = []

        for train_idx, val_idx in kf.split(x_train):
            x_subtrain, x_val = x_train[train_idx], x_train[val_idx]
            y_subtrain, y_val = y_train[train_idx], y_train[val_idx]

            model = build_deep_fm(
                input_dim=x_train.shape[1],
                embed_dim=embed_dim,
                dnn_units=dnn_units,
                dropout_rate=dropout_rate,
                activation=activation,
                use_batch_norm=use_batch_norm,
                learning_rate=learning_rate
            )

            model.fit(
                x_subtrain, y_subtrain,
                validation_data=(x_val, y_val),
                epochs=30,
                batch_size=32,
                verbose=0,
                callbacks=[early_stop]
            )

            y_val_pred = (model.predict(x_val) > 0.5).astype(int)
            f1 = f1_score(y_val, y_val_pred)
            f1_scores.append(f1)

        return np.mean(f1_scores)

    else:
        raise ValueError("validation_mode must be either 'split' or 'kfold'")


# Wide&Deep æ¨¡å‹
def build_wide_deep(
        input_dim,
        deep_units=None,
        dropout_rate=0.5,
        use_batch_norm=False,
        wide_activation='linear',  # å¯å°è¯• linear, relu, sigmoid, tanh
        activation='relu',  # å¯å°è¯• 'relu', 'tanh', 'selu'
        learning_rate=None
):
    """
    æ„å»ºä¸€ä¸ªWide&Deepæ¨¡å‹ã€‚

    å‚æ•°:
    - input_dim: è¾“å…¥ç»´åº¦ã€‚
    - deep_units: Deepéƒ¨åˆ†çš„å•å…ƒæ•°åˆ—è¡¨ï¼Œæ¯ä¸€é¡¹ä»£è¡¨ç›¸åº”å±‚çš„å•å…ƒæ•°ã€‚
    - dropout_rate: Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ 0.3 ~ 0.5ï¼Œå°æ ·æœ¬å»ºè®®åå¤§
    - use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ã€‚
        æ·±å±‚ç½‘ç»œï¼ˆ3å±‚ä»¥ä¸Šï¼‰	    âœ… æ¨è
        ç‰¹å¾åˆ†å¸ƒå·®å¼‚å¤§ï¼ˆå¦‚é™ç»´åï¼‰	âœ… æ¨è
        å°æ ·æœ¬+è¿‡æ‹Ÿåˆæ˜æ˜¾	        âŒ å¯å…ˆä¸ç”¨ï¼Œå…ˆç”¨ Dropout
    - wide_activation: Wideéƒ¨åˆ†çš„æ¿€æ´»å‡½æ•°ã€‚
        'linear'ï¼ˆé»˜è®¤ï¼‰	ä¸åŠ ä»»ä½•å˜æ¢ï¼Œè¾“å‡º âˆ‘wx+b     æ ‡å‡†åšæ³•
        'relu'	        çº¿æ€§ç»“æœè´Ÿæ•°ä¼šå˜ä¸º 0	      å¼ºè¡Œéè´Ÿï¼Œä¸æ¨è
        'sigmoid'	    å°†è¾“å‡ºå‹ç¼©åˆ°(0,1)åŒºé—´        é€‚ç”¨äºæ¦‚ç‡è¡¨è¾¾ï¼Œä½†é€šå¸¸åœ¨æœ€ç»ˆå±‚ç”¨
        'tanh'	        å‹ç¼©åˆ°(-1,1)               æå°‘ç”¨äº Wide
    - activation: Deepéƒ¨åˆ†çš„æ¿€æ´»å‡½æ•°ã€‚
        'relu'	        é»˜è®¤é¦–é€‰        ç®€å•é«˜æ•ˆï¼Œæ”¶æ•›å¿«
        'tanh'	        æœ‰ä¸­å¿ƒåŒ–        é€‚åˆå¯¹ç§°é—®é¢˜	æœ‰æ¢¯åº¦é¥±å’Œé£é™©
        'selu'	        è‡ªå½’ä¸€åŒ–        é€‚åˆæ·±ç½‘ç»œ	å¯¹è¾“å…¥æ•æ„Ÿï¼Œéœ€è¦æ­é…ç‰¹å®šåˆå§‹åŒ–
    - learning_rate: å­¦ä¹ ç‡ã€‚
        å°æ ·æœ¬ + å°ç½‘ç»œ	    0.01 ~ 0.001
        ä¸­ç­‰æ ·æœ¬ + ä¸­æ·±ç½‘ç»œ	0.001 ~ 0.0005ï¼ˆæ¨èï¼‰
        å¤§æ ·æœ¬ + æ·±æ¨¡å‹	    0.0005 ~ 0.0001

    è¿”å›:
    ç¼–è¯‘åçš„Wide&Deepæ¨¡å‹ã€‚
    """
    # å®šä¹‰æ¨¡å‹è¾“å…¥
    inputs = Input(shape=(input_dim,))
    # é»˜è®¤Deepéƒ¨åˆ†çš„ç»“æ„ä¸º[128, 64, 32]
    if deep_units is None:
        deep_units = [128, 64, 32]

    # Wideéƒ¨åˆ† ğŸ‘‰ å»ºæ¨¡â€œè®°å¿†æ€§â€ç‰¹å¾å’Œä½é˜¶å…³ç³»
    wide_output = Dense(1, activation=wide_activation)(inputs)

    # Deepéƒ¨åˆ† ğŸ‘‰ æ•æ‰é«˜é˜¶ç‰¹å¾ç»„åˆçš„éçº¿æ€§å…³ç³» (å¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œ MLP)
    x = inputs
    for units in deep_units:
        x = Dense(units)(x)  # å…¨è¿æ¥å±‚
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ·»åŠ æ‰¹é‡å½’ä¸€åŒ–å±‚
        if use_batch_norm:
            x = BatchNormalization()(x)  # å°†æ¯å±‚è¾“å‡ºæ ‡å‡†åŒ–ä¸ºå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
        # æ·»åŠ æ¿€æ´»å‡½æ•°å±‚
        x = Activation(activation)(x)
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ·»åŠ Dropoutå±‚
        if dropout_rate > 0:
            x = Dropout(dropout_rate)(x)  # éšæœºâ€œå±è”½â€ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    # Deepéƒ¨åˆ†çš„æ‰“åˆ†å±‚,
    deep_output = Dense(1, activation='linear')(x)

    # åˆå¹¶ Wide&Deep çš„è¾“å‡º ğŸ‘‰ Wideä¸Deepæ˜¯äº’è¡¥å…³ç³», Wideæ˜¯ä½é˜¶/çº¿æ€§/è¡¨è¾¾èƒ½åŠ›å¼±, Deepæ˜¯é«˜é˜¶/éçº¿æ€§/å®¹æ˜“è¿‡æ‹Ÿåˆ
    combined = Add()([wide_output, deep_output])
    # æœ€ç»ˆè¾“å‡ºå±‚ï¼Œç”¨äºäºŒåˆ†ç±»é—®é¢˜
    output = Dense(1, activation='sigmoid')(combined)

    # æ„å»ºæ¨¡å‹
    if learning_rate is None:  # å¦‚æœæ²¡æœ‰æä¾›å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„åŠ¨æ€å­¦ä¹ ç‡
        lr_schedule = ExponentialDecay(
            initial_learning_rate=0.0001,
            decay_steps=10000,
            decay_rate=0.1,
            staircase=True
        )
        optimizer = Adam(learning_rate=lr_schedule)
    else:  # å¦‚æœæä¾›äº†å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„å­¦ä¹ ç‡
        optimizer = Adam(learning_rate=learning_rate)

    # å®šä¹‰æ¨¡å‹å¹¶ç¼–è¯‘
    model = Model(inputs=inputs, outputs=output)
    model.compile(
        loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )
    return model


# Optunaè¯•éªŒçš„ç›®æ ‡å‡½æ•°, ç”¨äºä¼˜åŒ–Wide&Deepæ¨¡å‹çš„è¶…å‚æ•°
def objective_wide_deep(trial, n_splits=5):
    """
    ä½¿ç”¨ K æŠ˜äº¤å‰éªŒè¯è¯„ä¼° Wide&Deep æ¨¡å‹çš„æ€§èƒ½ï¼ˆf1-scoreï¼‰ã€‚
    """

    # è¶…å‚æ•°æœç´¢ç©ºé—´å®šä¹‰
    dropout_rate = trial.suggest_float("dropout_rate", 0.3, 0.6)
    use_batch_norm = trial.suggest_categorical("use_batch_norm", [True, False])
    wide_activation = trial.suggest_categorical("wide_activation", ['linear', 'sigmoid'])
    activation = trial.suggest_categorical("activation", ['relu', 'tanh'])
    learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
    deep_units = trial.suggest_categorical("deep_units", [
        [128, 64],
        [128, 64, 32],
        [128, 64, 32, 16],
        [64, 32],
        [64, 32, 16],
        [64, 32, 16, 8],
        [32, 16],
        [32, 16, 8],
    ])

    # KæŠ˜äº¤å‰éªŒè¯è®¾ç½®
    kf = KFold(
        n_splits=n_splits,  # KæŠ˜äº¤å‰éªŒè¯çš„æŠ˜æ•°
        shuffle=True,  # æ‰“ä¹±æ•°æ®ï¼Œé¿å…åå€š
        random_state=42  # è¿”å›å¹³å‡ F1 åˆ†æ•°ä½œä¸ºè¯„ä»·æŒ‡æ ‡
    )
    f1_scores = []

    for train_index, val_index in kf.split(x_train):
        x_subtrain, x_val = x_train[train_index], x_train[val_index]
        y_subtrain, y_val = y_train[train_index], y_train[val_index]

        # æ„å»ºæ¨¡å‹
        model = build_wide_deep(
            input_dim=x_train.shape[1],
            deep_units=deep_units,
            dropout_rate=dropout_rate,
            use_batch_norm=use_batch_norm,
            wide_activation=wide_activation,
            activation=activation,
            learning_rate=learning_rate
        )

        # æ—©åœå›è°ƒ
        early_stop = EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=0
        )

        # è®­ç»ƒæ¨¡å‹
        model.fit(
            x_subtrain, y_subtrain,
            validation_data=(x_val, y_val),
            epochs=30,
            batch_size=32,
            verbose=0,
            callbacks=[early_stop]
        )

        # éªŒè¯é›†è¯„ä¼°
        y_val_pred = (model.predict(x_val) > 0.5).astype(int)
        f1 = f1_score(y_val, y_val_pred)
        f1_scores.append(f1)

    # è¿”å›å¹³å‡ F1 ä½œä¸ºç›®æ ‡ä¼˜åŒ–æŒ‡æ ‡
    return np.mean(f1_scores)


# DCN æ¨¡å‹ (æ·±åº¦äº¤å‰ç½‘ç»œ)
def build_dcn(input_dim,
              cross_layers=5,
              deep_units: list = None,
              dropout_rate: float = 0.5,
              learning_rate: float = None,
              use_batch_norm=False
              ):
    """
    æ„å»ºæ·±åº¦äº¤å‰ç½‘ç»œï¼ˆDeep & Cross Network, DCNï¼‰æ¨¡å‹ã€‚

    :param input_dim: è¾“å…¥ç‰¹å¾çš„ç»´åº¦ã€‚
    :param cross_layers: äº¤å‰å±‚çš„æ•°é‡ï¼Œé»˜è®¤ä¸º5å±‚ã€‚
            å°æ ·æœ¬ï¼ˆå‡ åƒï¼‰+ é«˜ç»´ï¼ˆ>200ç»´ï¼‰	    1 ~ 2 å±‚ï¼ˆè¿‡å¤šå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
            ä¸­æ ·æœ¬ï¼ˆå‡ ä¸‡ï¼‰+ ä¸­ç»´ï¼ˆå‡ å ~ ç™¾ï¼‰	2 ~ 3 å±‚ï¼ˆå…¸å‹è®¾ç½®ï¼‰
            å¤§æ ·æœ¬ï¼ˆç™¾ä¸‡ï¼‰+ ä½ç»´ï¼ˆ<50ï¼‰	        3 ~ 4 å±‚å¯å°è¯•ï¼ˆæå‡é«˜é˜¶è¡¨è¾¾ï¼‰
    :param deep_units: æ·±åº¦éƒ¨åˆ†æ¯å±‚çš„å•å…ƒæ•°åˆ—è¡¨ï¼Œé»˜è®¤ä¸º[128, 64, 32]ã€‚
            å°æ ·æœ¬ï¼ˆå‡ åƒï¼‰+ é«˜ç»´ï¼ˆ>200ç»´ï¼‰	    [64, 32] æˆ– [128, 64, 32]
            ä¸­æ ·æœ¬ï¼ˆå‡ ä¸‡ï¼‰	                [128, 64, 32] æˆ– [256, 128, 64]
            å¤§æ ·æœ¬ï¼ˆç™¾ä¸‡ï¼‰	                [512, 256, 128] æˆ–æ›´æ·±
    :param dropout_rate: Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé»˜è®¤ä¸º0.5ã€‚
            å°æ ·æœ¬ï¼ˆå‡ åƒï¼‰     0.4 ~ 0.6
            ä¸­æ ·æœ¬ï¼ˆå‡ ä¸‡ï¼‰     0.3 ~ 0.5
            å¤§æ ·æœ¬ï¼ˆç™¾ä¸‡ï¼‰     0.2 ~ 0.4
    :param learning_rate: å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º None è¡¨ç¤ºä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡ã€‚
            å°æ ·æœ¬ + å°ç½‘ç»œ	    0.01 ~ 0.001
            ä¸­ç­‰æ ·æœ¬ + ä¸­æ·±ç½‘ç»œ	0.001 ~ 0.0005ï¼ˆæ¨èï¼‰
            å¤§æ ·æœ¬ + æ·±æ¨¡å‹	    0.0005 ~ 0.0001
    :param use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰ï¼Œé»˜è®¤ä¸ºFalseã€‚
            æ·±å±‚ç½‘ç»œï¼ˆ3å±‚ä»¥ä¸Šï¼‰	    âœ… æ¨è
            ç‰¹å¾åˆ†å¸ƒå·®å¼‚å¤§ï¼ˆå¦‚é™ç»´åï¼‰	âœ… æ¨è
            å°æ ·æœ¬+è¿‡æ‹Ÿåˆæ˜æ˜¾	        âŒ å¯å…ˆä¸ç”¨ï¼Œå…ˆç”¨ Dropout
    :return: æ„å»ºå¥½çš„DCNæ¨¡å‹ã€‚
    """
    # å®šä¹‰æ¨¡å‹è¾“å…¥
    inputs = Input(shape=(input_dim,))
    x0 = inputs
    xl = inputs

    # å¦‚æœæœªæä¾›deep_unitsï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
    if deep_units is None:
        deep_units = [128, 64, 32]

    # Cross Part ğŸ‘‰ æ˜¾å¼å»ºæ¨¡ç‰¹å¾ä¹‹é—´çš„äº¤å‰ç»„åˆé¡¹
    for _ in range(cross_layers):
        xl_w = Dense(input_dim)(xl)
        cross = Multiply()([x0, xl_w])
        xl = Add()([cross, xl])

    # Deep Part ğŸ‘‰ æ•æ‰é«˜é˜¶ç‰¹å¾ç»„åˆçš„éçº¿æ€§å…³ç³» (å¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œ MLP)
    deep = inputs
    for units in deep_units:
        deep = Dense(units, activation='relu')(deep)
        # æ ¹æ®use_batch_normå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–
        if use_batch_norm:
            from tensorflow.keras.layers import BatchNormalization
            deep = BatchNormalization()(deep)
        # æ ¹æ®dropout_rateå‚æ•°å†³å®šæ˜¯å¦åº”ç”¨dropout
        if dropout_rate > 0:
            from tensorflow.keras.layers import Dropout
            deep = Dropout(dropout_rate)(deep)

    # Final
    concat = Concatenate()([xl, deep])
    output = Dense(1, activation='sigmoid')(concat)

    # æ„å»ºæ¨¡å‹
    if learning_rate is None:  # å¦‚æœæ²¡æœ‰æä¾›å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„åŠ¨æ€å­¦ä¹ ç‡
        lr_schedule = ExponentialDecay(
            initial_learning_rate=0.0001,
            decay_steps=10000,
            decay_rate=0.1,
            staircase=True
        )
        optimizer = Adam(learning_rate=lr_schedule)
    else:  # å¦‚æœæä¾›äº†å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„å­¦ä¹ ç‡
        optimizer = Adam(learning_rate=learning_rate)

    # å®šä¹‰æ¨¡å‹å¹¶ç¼–è¯‘
    model = Model(inputs=inputs, outputs=output)
    model.compile(
        loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )
    return model


# Optunaè¯•éªŒçš„ç›®æ ‡å‡½æ•°, ç”¨äºä¼˜åŒ–DCNæ¨¡å‹çš„è¶…å‚æ•°
def objective_dcn(trial, validation_mode='kfold', n_splits=5):
    """
    Optunaè¯•éªŒçš„ç›®æ ‡å‡½æ•°ï¼Œç”¨äºä¼˜åŒ–DCNæ¨¡å‹çš„è¶…å‚æ•°ã€‚

    å‚æ•°:
    - trial: Optunaçš„è¯•éªŒå¯¹è±¡ï¼Œç”¨äºé€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
    - validation_mode: é€‰æ‹© 'split' æˆ– 'kfold'ï¼Œç”¨äºé€‰æ‹©éªŒè¯æ–¹å¼ã€‚

    è¿”å›:
    - å¹³å‡F1åˆ†æ•°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    """

    # è¶…å‚æ•°ç©ºé—´å®šä¹‰
    cross_layers = trial.suggest_int('cross_layers', 2, 6)
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.7)
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])
    deep_units = trial.suggest_categorical('deep_units', [
        [128, 64],
        [128, 64, 32],
        [128, 64, 32, 16],
        [64, 32],
        [64, 32, 16],
        [64, 32, 16, 8],
        [32, 16],
        [32, 16, 8],
    ])

    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)

    # æ¨¡å‹è®­ç»ƒä¸éªŒè¯
    if validation_mode == 'split':
        # å›ºå®šåˆ’åˆ†éªŒè¯é›†
        x_subtrain, x_val, y_subtrain, y_val = train_test_split(
            x_train, y_train, test_size=0.1, shuffle=False
        )

        model = build_dcn(
            input_dim=x_train.shape[1],
            cross_layers=cross_layers,
            deep_units=deep_units,
            dropout_rate=dropout_rate,
            learning_rate=learning_rate,
            use_batch_norm=use_batch_norm
        )

        model.fit(
            x_subtrain, y_subtrain,
            validation_data=(x_val, y_val),
            epochs=30,
            batch_size=32,
            verbose=0,
            callbacks=[early_stop]
        )

        y_val_pred = (model.predict(x_val) > 0.5).astype(int)
        return f1_score(y_val, y_val_pred)

    elif validation_mode == 'kfold':
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
        f1_scores = []

        for train_idx, val_idx in kf.split(x_train):
            x_subtrain, x_val = x_train[train_idx], x_train[val_idx]
            y_subtrain, y_val = y_train[train_idx], y_train[val_idx]

            model = build_dcn(
                input_dim=x_train.shape[1],
                cross_layers=cross_layers,
                deep_units=deep_units,
                dropout_rate=dropout_rate,
                learning_rate=learning_rate,
                use_batch_norm=use_batch_norm
            )

            model.fit(
                x_subtrain, y_subtrain,
                validation_data=(x_val, y_val),
                epochs=30,
                batch_size=32,
                verbose=0,
                callbacks=[early_stop]
            )

            y_val_pred = (model.predict(x_val) > 0.5).astype(int)
            f1 = f1_score(y_val, y_val_pred)
            f1_scores.append(f1)

        return np.mean(f1_scores)

    else:
        raise ValueError("validation_mode must be either 'split' or 'kfold'")


# è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
def train_and_evaluate(model, x_train, y_train, x_test, y_test, epochs=10, batch_size=32):
    """
    è®­ç»ƒå¹¶è¯„ä¼°ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

    å‚æ•°:
    - model: è¦è®­ç»ƒå’Œè¯„ä¼°çš„æ¨¡å‹ã€‚
    - x_train: è®­ç»ƒæ•°æ®ç‰¹å¾ã€‚
    - y_train: è®­ç»ƒæ•°æ®æ ‡ç­¾ã€‚
    - x_test: æµ‹è¯•æ•°æ®ç‰¹å¾ã€‚
    - y_test: æµ‹è¯•æ•°æ®æ ‡ç­¾ã€‚
    - epochs: è®­ç»ƒçš„è½®æ•°ï¼Œé»˜è®¤ä¸º10ã€‚
    - batch_size: æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡çš„å¤§å°ï¼Œé»˜è®¤ä¸º32ã€‚

    æ­¤å‡½æ•°æ²¡æœ‰è¿”å›å€¼ï¼Œä½†ä¼šåœ¨æ§åˆ¶å°è¾“å‡ºæ¨¡å‹çš„è¯„ä¼°æŠ¥å‘Šã€‚
    """
    # è®­ç»ƒæ¨¡å‹
    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2, validation_split=0.1)

    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹å¹¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶åˆ†ç±»
    pred = (model.predict(x_test) > 0.5).astype(int)

    # è¾“å‡ºåˆ†ç±»æŠ¥å‘Š
    print(classification_report(y_test, pred))


if __name__ == '__main__':
    from ReturnClassification.train_and_test import main_data_prepare

    # æ•°æ®è¯»å–ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä»¥åŠ åŸå§‹æŒ‡æ ‡æ•°æ®
    x_train, y_train, x_test, y_test, metrics_data = main_data_prepare(
        the_fund_code='510050.SH',  # æŒ‡å®šåŸºé‡‘ä»£ç ï¼Œæ­¤å¤„ä¸º '510050.SH'
        n_days=10,
        folder_path='../Data',  # åŸºé‡‘ä»·æ ¼æ•°æ®çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º '../Data'
        metrics_folder='../Data/Metrics',  # åŸºé‡‘æŒ‡æ ‡æ•°æ®çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º '../Data/Metrics'
        train_start=None,  # è®­ç»ƒé›†å¼€å§‹æ—¥æœŸï¼Œå¦‚æœä¸º Noneï¼Œåˆ™ä»æ•°æ®çš„æœ€æ—©æ—¥æœŸå¼€å§‹
        train_end='2024-11-30',  # è®­ç»ƒé›†ç»“æŸæ—¥æœŸï¼ŒæŒ‡å®šä¸º '2024-11-30'
        test_start='2024-12-01',  # æµ‹è¯•é›†å¼€å§‹æ—¥æœŸï¼ŒæŒ‡å®šä¸º '2024-12-01'
        test_end='2025-04-30',  # æµ‹è¯•é›†ç»“æŸæ—¥æœŸï¼ŒæŒ‡å®šä¸º '2025-04-30'
        nan_method='drop',  # å¤„ç†ç¼ºå¤±å€¼çš„æ–¹æ³•ï¼Œé»˜è®¤ä¸º 'drop'ï¼ˆåˆ é™¤ç¼ºå¤±å€¼ï¼‰ï¼Œå¯é€‰ 'median' æˆ– 'mean'
        standardize_method='zscore',  # æŒ‡æ ‡æ ‡å‡†åŒ–çš„æ–¹æ³•,å¯é€‰: 'minmax', 'zscore', 'both', 'none'ã€‚
        basic_data_as_metric=True,  # æ˜¯å¦å°†åŸºæœ¬æ•°æ®ï¼ˆå¦‚å¼€ç›˜ä»·ã€æ”¶ç›˜ä»·ã€äº¤æ˜“é‡ç­‰ï¼‰ä½œä¸ºç‰¹å¾æ•°æ®ï¼Œé»˜è®¤ä¸º True
        return_threshold=0.0,  # æ ‡ç­¾ç”Ÿæˆæ–¹æ³•ï¼Œæœªæ¥æ”¶ç›Šç‡å¤§äº 0.01 çš„æ ·æœ¬æ ‡è®°ä¸º 1ï¼Œå¦åˆ™ä¸º 0
        dim_reduction=True,  # æ˜¯å¦è¿›è¡Œç‰¹å¾é™ç»´ï¼Œé»˜è®¤ä¸º False
        n_components=None,  # PCAé™ç»´çš„ç›®æ ‡ç»´åº¦ï¼Œé»˜è®¤ä¸º 100
    )

    # æ•°æ®é¢„å¤„ç†
    # æ•°æ®é¢„å¤„ç†ï¼ˆæ³¨æ„ç»Ÿä¸€scalerï¼‰
    scaler = MinMaxScaler()
    x_train_scaled = scaler.fit_transform(x_train)
    x_test_scaled = scaler.transform(x_test)
    y_train_values = y_train.values
    y_test_values = y_test.values

    input_dim = x_train.shape[1]

    ''' 1) DeepFM æ¨¡å‹ '''
    print(" --- " * 20)
    print("Training DeepFM...")
    # ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
    study = optuna.create_study(direction='maximize')
    study.optimize(
        lambda trial: objective_deep_fm(trial, validation_mode='kfold'),
        n_trials=15
    )
    print("Best trial:", study.best_trial.params)  # è¾“å‡ºæœ€ä¼˜è¶…å‚æ•°
    best_trial = study.best_trial.params
    # best_trial = {'embed_dim': 32, 'dropout_rate': 0.4152850750188673, 'learning_rate': 0.001492202547205302, 'activation': 'relu', 'use_batch_norm': False, 'dnn_units': [64, 32, 16, 8]}

    # ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    deep_fm_model = build_deep_fm(
        input_dim,
        embed_dim=best_trial['embed_dim'],
        dnn_units=best_trial['dnn_units'],
        dropout_rate=best_trial['dropout_rate'],
        activation=best_trial['activation'],
        use_batch_norm=best_trial['use_batch_norm'],
        learning_rate=best_trial['learning_rate'],
    )

    # è®­ç»ƒå¹¶è¯„ä¼°DCNæ¨¡å‹
    train_and_evaluate(
        deep_fm_model,  # è¦è®­ç»ƒå’Œè¯„ä¼°çš„DCNæ¨¡å‹ï¼Œç”± `build_dcn` å‡½æ•°æ„å»ºï¼Œä½¿ç”¨äº† Optuna ä¼˜åŒ–åçš„è¶…å‚æ•°ã€‚
        x_train_scaled,  # è®­ç»ƒæ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†ï¼ˆå¦‚å½’ä¸€åŒ–ï¼‰åçš„ç‰¹å¾çŸ©é˜µï¼Œç¡®ä¿è¾“å…¥æ•°æ®å…·æœ‰ç›¸åŒçš„å°ºåº¦ã€‚
        y_train_values,  # è®­ç»ƒæ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºè®­ç»ƒç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºç›‘ç£å­¦ä¹ è¿‡ç¨‹ã€‚
        x_test_scaled,  # æµ‹è¯•æ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†åçš„ç‰¹å¾çŸ©é˜µï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
        y_test_values,  # æµ‹è¯•æ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºæµ‹è¯•ç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æŒ‡æ ‡ã€‚
        epochs=10,  # æŒ‡å®šæ¨¡å‹è®­ç»ƒçš„è½®æ•°ï¼ˆepochï¼‰ï¼Œå³æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šå®Œæ•´è®­ç»ƒçš„æ¬¡æ•°ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
        batch_size=32  # æŒ‡å®šæ¯ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰çš„å¤§å°ï¼Œå³æ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
    )

    # ''' 2) Wide & Deep æ¨¡å‹ '''
    print(" --- " * 20)
    print("\nTraining Wide & Deep...")
    # ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
    study = optuna.create_study(direction='maximize')
    study.optimize(objective_wide_deep,
                   n_trials=15  # è®¾ç½®è¯•éªŒæ¬¡æ•°
                   )
    print("Best trial:", study.best_trial.params)  # è¾“å‡ºæœ€ä¼˜è¶…å‚æ•°
    best_trial = study.best_trial.params
    # best_trial = {'dropout_rate': 0.31110760370578694, 'use_batch_norm': True, 'wide_activation': 'linear', 'activation': 'relu', 'learning_rate': 0.0020187999563270574, 'deep_units': [64, 32]}

    # ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    wd_model = build_wide_deep(
        input_dim=input_dim,  # è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼Œå³æ¨¡å‹è¾“å…¥çš„å¤§å°ã€‚
        deep_units=best_trial['deep_units'],
        dropout_rate=best_trial['dropout_rate'],
        use_batch_norm=best_trial['use_batch_norm'],
        wide_activation=best_trial['wide_activation'],
        activation=best_trial['activation'],
        learning_rate=best_trial['learning_rate'],
    )
    # è®­ç»ƒå¹¶è¯„ä¼°Wide&Deepæ¨¡å‹
    train_and_evaluate(wd_model,  # è¦è®­ç»ƒå’Œè¯„ä¼°çš„Wide&Deepæ¨¡å‹ï¼Œç”± `build_wide_deep` å‡½æ•°æ„å»ºï¼Œä½¿ç”¨äº† Optuna ä¼˜åŒ–åçš„è¶…å‚æ•°ã€‚
                       x_train_scaled,  # è®­ç»ƒæ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†ï¼ˆå¦‚å½’ä¸€åŒ–ï¼‰åçš„ç‰¹å¾çŸ©é˜µï¼Œç¡®ä¿è¾“å…¥æ•°æ®å…·æœ‰ç›¸åŒçš„å°ºåº¦ã€‚
                       y_train_values,  # è®­ç»ƒæ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºè®­ç»ƒç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºç›‘ç£å­¦ä¹ è¿‡ç¨‹ã€‚
                       x_test_scaled,  # æµ‹è¯•æ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†åçš„ç‰¹å¾çŸ©é˜µï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
                       y_test_values,  # æµ‹è¯•æ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºæµ‹è¯•ç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æŒ‡æ ‡ã€‚
                       epochs=10,  # æŒ‡å®šæ¨¡å‹è®­ç»ƒçš„è½®æ•°ï¼ˆepochï¼‰ï¼Œå³æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šå®Œæ•´è®­ç»ƒçš„æ¬¡æ•°ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
                       batch_size=32  # æŒ‡å®šæ¯ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰çš„å¤§å°ï¼Œå³æ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
                       )

    ''' 3) NCD æ¨¡å‹ '''
    print(" --- " * 20)
    print("\nTraining DCN...")
    # ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
    study = optuna.create_study(direction='maximize')
    study.optimize(
        lambda trial: objective_dcn(trial, validation_mode='kfold'),
        n_trials=15
    )
    print("Best trial:", study.best_trial.params)  # è¾“å‡ºæœ€ä¼˜è¶…å‚æ•°
    best_trial = study.best_trial.params
    # best_trial = {'cross_layers': 2, 'dropout_rate': 0.2043031648581299, 'learning_rate': 0.0009367655839776305, 'use_batch_norm': False, 'deep_units': [128, 64]}

    # ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    dcn_model = build_dcn(
        input_dim=input_dim,  # è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼Œå³æ¨¡å‹è¾“å…¥çš„å¤§å°ã€‚
        cross_layers=best_trial['cross_layers'],  # äº¤å‰å±‚çš„æ•°é‡ï¼Œæ§åˆ¶æ¨¡å‹ä¸­æ˜¾å¼å»ºæ¨¡ç‰¹å¾äº¤å‰ç»„åˆçš„å±‚æ•°ã€‚
        deep_units=best_trial['deep_units'],  # æ·±åº¦éƒ¨åˆ†æ¯å±‚çš„å•å…ƒæ•°åˆ—è¡¨ï¼Œå®šä¹‰æ·±åº¦ç½‘ç»œçš„ç»“æ„ã€‚
        dropout_rate=best_trial['dropout_rate'],  # Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œéšæœºä¸¢å¼ƒç¥ç»å…ƒçš„æ¯”ä¾‹ã€‚
        learning_rate=best_trial['learning_rate'],  # å­¦ä¹ ç‡ï¼Œæ§åˆ¶ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°çš„é€Ÿåº¦ã€‚
        use_batch_norm=best_trial['use_batch_norm'],  # æ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒå’Œæé«˜æ¨¡å‹ç¨³å®šæ€§ã€‚
    )

    # è®­ç»ƒå¹¶è¯„ä¼°DCNæ¨¡å‹
    train_and_evaluate(
        dcn_model,  # è¦è®­ç»ƒå’Œè¯„ä¼°çš„DCNæ¨¡å‹ï¼Œç”± `build_dcn` å‡½æ•°æ„å»ºï¼Œä½¿ç”¨äº† Optuna ä¼˜åŒ–åçš„è¶…å‚æ•°ã€‚
        x_train_scaled,  # è®­ç»ƒæ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†ï¼ˆå¦‚å½’ä¸€åŒ–ï¼‰åçš„ç‰¹å¾çŸ©é˜µï¼Œç¡®ä¿è¾“å…¥æ•°æ®å…·æœ‰ç›¸åŒçš„å°ºåº¦ã€‚
        y_train_values,  # è®­ç»ƒæ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºè®­ç»ƒç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºç›‘ç£å­¦ä¹ è¿‡ç¨‹ã€‚
        x_test_scaled,  # æµ‹è¯•æ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†åçš„ç‰¹å¾çŸ©é˜µï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
        y_test_values,  # æµ‹è¯•æ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºæµ‹è¯•ç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æŒ‡æ ‡ã€‚
        epochs=10,  # æŒ‡å®šæ¨¡å‹è®­ç»ƒçš„è½®æ•°ï¼ˆepochï¼‰ï¼Œå³æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šå®Œæ•´è®­ç»ƒçš„æ¬¡æ•°ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
        batch_size=32  # æŒ‡å®šæ¯ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰çš„å¤§å°ï¼Œå³æ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
    )
