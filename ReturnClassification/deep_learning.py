# -*- encoding: utf-8 -*-
"""
@File: test.py.py
@Modify Time: 2025/4/21 19:35
@Author: Kevin-Chen
@Descriptions:
"""
import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report
from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Flatten, Add, Multiply, Reshape
from tensorflow.keras.layers import Dropout, BatchNormalization, Activation, Reshape, Lambda
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from sklearn.metrics import f1_score
import tensorflow as tf


# è‡ªå®šä¹‰FMå±‚ï¼ˆä½¿ç”¨Lambdaå°è£…tfæ“ä½œï¼‰
def fm_cross_layer(embed):
    summed_features_emb = tf.reduce_sum(embed, axis=1)
    summed_features_emb_square = tf.square(summed_features_emb)

    squared_features_emb = tf.square(embed)
    squared_sum_features_emb = tf.reduce_sum(squared_features_emb, axis=1)

    fm_output = 0.5 * (summed_features_emb_square - squared_sum_features_emb)
    return fm_output


def build_deep_fm(input_dim, embed_dim=8):
    inputs = Input(shape=(input_dim,))

    # Embedding
    embed = Dense(input_dim * embed_dim, activation='linear')(inputs)
    embed = Reshape((input_dim, embed_dim))(embed)

    # FMäº¤å‰é¡¹ (ä½¿ç”¨Lambdaå±‚)
    fm_output = Lambda(fm_cross_layer)(embed)

    # DNN
    dnn_input = Flatten()(embed)
    dnn_output = Dense(128, activation='relu')(dnn_input)
    dnn_output = Dense(64, activation='relu')(dnn_output)

    # ç»„åˆFMå’ŒDNN
    concat = Concatenate()([fm_output, dnn_output])
    output = Dense(1, activation='sigmoid')(concat)

    model = Model(inputs=inputs, outputs=output)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


# Wide&Deep æ¨¡å‹
def build_wide_deep(
        input_dim,
        deep_units=None,
        dropout_rate=0.5,
        use_batch_norm=False,
        wide_activation='linear',  # å¯å°è¯• linear, relu, sigmoid, tanh
        activation='relu',  # å¯å°è¯• 'relu', 'tanh', 'selu'
        learning_rate=None
):
    """
    æ„å»ºä¸€ä¸ªWide&Deepæ¨¡å‹ã€‚

    å‚æ•°:
    - input_dim: è¾“å…¥ç»´åº¦ã€‚
    - deep_units: Deepéƒ¨åˆ†çš„å•å…ƒæ•°åˆ—è¡¨ï¼Œæ¯ä¸€é¡¹ä»£è¡¨ç›¸åº”å±‚çš„å•å…ƒæ•°ã€‚
    - dropout_rate: Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ 0.3 ~ 0.5ï¼Œå°æ ·æœ¬å»ºè®®åå¤§
    - use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ã€‚
        æ·±å±‚ç½‘ç»œï¼ˆ3å±‚ä»¥ä¸Šï¼‰	    âœ… æ¨è
        ç‰¹å¾åˆ†å¸ƒå·®å¼‚å¤§ï¼ˆå¦‚é™ç»´åï¼‰	âœ… æ¨è
        å°æ ·æœ¬+è¿‡æ‹Ÿåˆæ˜æ˜¾	        âŒ å¯å…ˆä¸ç”¨ï¼Œå…ˆç”¨ Dropout
    - wide_activation: Wideéƒ¨åˆ†çš„æ¿€æ´»å‡½æ•°ã€‚
        'linear'ï¼ˆé»˜è®¤ï¼‰	ä¸åŠ ä»»ä½•å˜æ¢ï¼Œè¾“å‡º âˆ‘wx+b     æ ‡å‡†åšæ³•
        'relu'	        çº¿æ€§ç»“æœè´Ÿæ•°ä¼šå˜ä¸º 0	      å¼ºè¡Œéè´Ÿï¼Œä¸æ¨è
        'sigmoid'	    å°†è¾“å‡ºå‹ç¼©åˆ°(0,1)åŒºé—´        é€‚ç”¨äºæ¦‚ç‡è¡¨è¾¾ï¼Œä½†é€šå¸¸åœ¨æœ€ç»ˆå±‚ç”¨
        'tanh'	        å‹ç¼©åˆ°(-1,1)               æå°‘ç”¨äº Wide
    - activation: Deepéƒ¨åˆ†çš„æ¿€æ´»å‡½æ•°ã€‚
        'relu'	        é»˜è®¤é¦–é€‰        ç®€å•é«˜æ•ˆï¼Œæ”¶æ•›å¿«
        'tanh'	        æœ‰ä¸­å¿ƒåŒ–        é€‚åˆå¯¹ç§°é—®é¢˜	æœ‰æ¢¯åº¦é¥±å’Œé£é™©
        'selu'	        è‡ªå½’ä¸€åŒ–        é€‚åˆæ·±ç½‘ç»œ	å¯¹è¾“å…¥æ•æ„Ÿï¼Œéœ€è¦æ­é…ç‰¹å®šåˆå§‹åŒ–
    - learning_rate: å­¦ä¹ ç‡ã€‚
        å°æ ·æœ¬ + å°ç½‘ç»œ	    0.01 ~ 0.001
        ä¸­ç­‰æ ·æœ¬ + ä¸­æ·±ç½‘ç»œ	0.001 ~ 0.0005ï¼ˆæ¨èï¼‰
        å¤§æ ·æœ¬ + æ·±æ¨¡å‹	    0.0005 ~ 0.0001

    è¿”å›:
    ç¼–è¯‘åçš„Wide&Deepæ¨¡å‹ã€‚
    """
    # å®šä¹‰æ¨¡å‹è¾“å…¥
    inputs = Input(shape=(input_dim,))
    # é»˜è®¤Deepéƒ¨åˆ†çš„ç»“æ„ä¸º[128, 64, 32]
    if deep_units is None:
        deep_units = [128, 64, 32]

    # Wideéƒ¨åˆ† ğŸ‘‰ å»ºæ¨¡â€œè®°å¿†æ€§â€ç‰¹å¾å’Œä½é˜¶å…³ç³»
    wide_output = Dense(1, activation=wide_activation)(inputs)

    # Deepéƒ¨åˆ† ğŸ‘‰ æ•æ‰é«˜é˜¶ç‰¹å¾ç»„åˆçš„éçº¿æ€§å…³ç³» (å¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œ MLP)
    x = inputs
    for units in deep_units:
        x = Dense(units)(x)  # å…¨è¿æ¥å±‚
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ·»åŠ æ‰¹é‡å½’ä¸€åŒ–å±‚
        if use_batch_norm:
            x = BatchNormalization()(x)  # å°†æ¯å±‚è¾“å‡ºæ ‡å‡†åŒ–ä¸ºå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
        # æ·»åŠ æ¿€æ´»å‡½æ•°å±‚
        x = Activation(activation)(x)
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ·»åŠ Dropoutå±‚
        if dropout_rate > 0:
            x = Dropout(dropout_rate)(x)  # éšæœºâ€œå±è”½â€ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    # Deepéƒ¨åˆ†çš„æ‰“åˆ†å±‚,
    deep_output = Dense(1, activation='linear')(x)

    # åˆå¹¶ Wide&Deep çš„è¾“å‡º ğŸ‘‰ Wideä¸Deepæ˜¯äº’è¡¥å…³ç³», Wideæ˜¯ä½é˜¶/çº¿æ€§/è¡¨è¾¾èƒ½åŠ›å¼±, Deepæ˜¯é«˜é˜¶/éçº¿æ€§/å®¹æ˜“è¿‡æ‹Ÿåˆ
    combined = Add()([wide_output, deep_output])
    # æœ€ç»ˆè¾“å‡ºå±‚ï¼Œç”¨äºäºŒåˆ†ç±»é—®é¢˜
    output = Dense(1, activation='sigmoid')(combined)

    # æ„å»ºæ¨¡å‹
    if learning_rate is None:  # å¦‚æœæ²¡æœ‰æä¾›å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„åŠ¨æ€å­¦ä¹ ç‡
        lr_schedule = ExponentialDecay(
            initial_learning_rate=0.0001,
            decay_steps=10000,
            decay_rate=0.1,
            staircase=True
        )
        optimizer = Adam(learning_rate=lr_schedule)
    else:  # å¦‚æœæä¾›äº†å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„å­¦ä¹ ç‡
        optimizer = Adam(learning_rate=learning_rate)

    # å®šä¹‰æ¨¡å‹å¹¶ç¼–è¯‘
    model = Model(inputs=inputs, outputs=output)
    model.compile(
        loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )
    return model

    # # å®šä¹‰æ¨¡å‹
    # model = Model(inputs=inputs, outputs=output)
    # # ç¼–è¯‘æ¨¡å‹ï¼Œä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°å’ŒAdamä¼˜åŒ–å™¨
    # model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])
    #
    # return model


# # Optunaè¯•éªŒçš„ç›®æ ‡å‡½æ•°, ç”¨äºä¼˜åŒ–Wide&Deepæ¨¡å‹çš„è¶…å‚æ•°
# def objective_wide_deep(trial):
#     """
#     å®šä¹‰Wide&Deepæ¨¡å‹çš„è¶…å‚æ•°ä¼˜åŒ–ç›®æ ‡å‡½æ•°ã€‚
#
#     å‚æ•°:
#     - trial: Optunaçš„è¯•éªŒå¯¹è±¡ï¼Œç”¨äºé€‰æ‹©è¶…å‚æ•°ã€‚
#
#     è¿”å›:
#     - éªŒè¯é›†ä¸Šçš„F1åˆ†æ•°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
#     """
#
#     # è¶…å‚æ•°æœç´¢ç©ºé—´å®šä¹‰
#     dropout_rate = trial.suggest_float("dropout_rate", 0.3, 0.6)
#     use_batch_norm = trial.suggest_categorical("use_batch_norm", [True, False])
#     wide_activation = trial.suggest_categorical("wide_activation", ['linear', 'sigmoid'])
#     activation = trial.suggest_categorical("activation", ['relu', 'tanh'])
#     learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
#     deep_units = trial.suggest_categorical("deep_units", [
#         [128, 64],
#         [128, 64, 32],
#         [64, 32],
#         [64, 32, 16],
#         [64, 32, 16, 8],
#         [32, 16],
#         [32, 16, 8],
#     ])
#
#     # æ„å»ºæ¨¡å‹
#     model = build_wide_deep(
#         input_dim=x_train.shape[1],
#         deep_units=deep_units,
#         dropout_rate=dropout_rate,
#         use_batch_norm=use_batch_norm,
#         wide_activation=wide_activation,
#         activation=activation,
#         learning_rate=learning_rate
#     )
#
#     # æ—©åœå›è°ƒï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
#     early_stop = EarlyStopping(
#         monitor='val_loss',  # ç›‘æ§çš„æŒ‡æ ‡ï¼Œæ­¤å¤„ä¸ºéªŒè¯é›†çš„æŸå¤±å€¼ï¼ˆval_lossï¼‰ï¼Œç”¨äºåˆ¤æ–­æ¨¡å‹æ˜¯å¦åœæ­¢è®­ç»ƒã€‚
#         patience=6,          # å®¹å¿è½®æ•°ï¼Œå½“éªŒè¯é›†æŸå¤±å€¼åœ¨è¿ç»­3ä¸ªepochå†…æ²¡æœ‰æ”¹å–„æ—¶ï¼Œè®­ç»ƒå°†æå‰ç»ˆæ­¢ã€‚
#         restore_best_weights=True,  # å½“è®­ç»ƒåœæ­¢æ—¶ï¼Œæ¢å¤åˆ°éªŒè¯é›†è¡¨ç°æœ€å¥½çš„æ¨¡å‹æƒé‡ã€‚
#         verbose=0            # æ—¥å¿—æ˜¾ç¤ºæ¨¡å¼ï¼Œ0è¡¨ç¤ºä¸è¾“å‡ºæ—¥å¿—ä¿¡æ¯ã€‚
#     )
#
#     # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆå›ºå®šä¸æ‰“ä¹±ï¼‰
#     x_subtrain, x_val, y_subtrain, y_val = train_test_split(
#         x_train, y_train, test_size=0.1, shuffle=False
#     )
#
#     # è®­ç»ƒæ¨¡å‹
#     model.fit(
#         x_subtrain, y_subtrain,
#         validation_data=(x_val, y_val),
#         epochs=30,
#         batch_size=32,
#         verbose=0,
#         callbacks=[early_stop]
#     )
#
#     # éªŒè¯é›†ä¸Šè¯„ä¼°æŒ‡æ ‡
#     y_val_pred = (model.predict(x_val) > 0.5).astype(int)
#     return f1_score(y_val, y_val_pred)


def objective_wide_deep(trial, n_splits=5):
    """
    ä½¿ç”¨ K æŠ˜äº¤å‰éªŒè¯è¯„ä¼° Wide&Deep æ¨¡å‹çš„æ€§èƒ½ï¼ˆf1-scoreï¼‰ã€‚
    """

    # è¶…å‚æ•°æœç´¢ç©ºé—´å®šä¹‰
    dropout_rate = trial.suggest_float("dropout_rate", 0.3, 0.6)
    use_batch_norm = trial.suggest_categorical("use_batch_norm", [True, False])
    wide_activation = trial.suggest_categorical("wide_activation", ['linear', 'sigmoid'])
    activation = trial.suggest_categorical("activation", ['relu', 'tanh'])
    learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
    deep_units = trial.suggest_categorical("deep_units", [
        [128, 64],
        [128, 64, 32],
        [128, 64, 32, 16],
        [64, 32],
        [64, 32, 16],
        [64, 32, 16, 8],
        [32, 16],
        [32, 16, 8],
    ])

    # KæŠ˜äº¤å‰éªŒè¯è®¾ç½®
    kf = KFold(
        n_splits=n_splits,  # KæŠ˜äº¤å‰éªŒè¯çš„æŠ˜æ•°
        shuffle=True,  # æ‰“ä¹±æ•°æ®ï¼Œé¿å…åå€š
        random_state=42  # è¿”å›å¹³å‡ F1 åˆ†æ•°ä½œä¸ºè¯„ä»·æŒ‡æ ‡
    )
    f1_scores = []

    for train_index, val_index in kf.split(x_train):
        x_subtrain, x_val = x_train[train_index], x_train[val_index]
        y_subtrain, y_val = y_train[train_index], y_train[val_index]

        # æ„å»ºæ¨¡å‹
        model = build_wide_deep(
            input_dim=x_train.shape[1],
            deep_units=deep_units,
            dropout_rate=dropout_rate,
            use_batch_norm=use_batch_norm,
            wide_activation=wide_activation,
            activation=activation,
            learning_rate=learning_rate
        )

        # æ—©åœå›è°ƒ
        early_stop = EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=0
        )

        # è®­ç»ƒæ¨¡å‹
        model.fit(
            x_subtrain, y_subtrain,
            validation_data=(x_val, y_val),
            epochs=30,
            batch_size=32,
            verbose=0,
            callbacks=[early_stop]
        )

        # éªŒè¯é›†è¯„ä¼°
        y_val_pred = (model.predict(x_val) > 0.5).astype(int)
        f1 = f1_score(y_val, y_val_pred)
        f1_scores.append(f1)

    # è¿”å›å¹³å‡ F1 ä½œä¸ºç›®æ ‡ä¼˜åŒ–æŒ‡æ ‡
    return np.mean(f1_scores)


# DCN æ¨¡å‹ (æ·±åº¦äº¤å‰ç½‘ç»œ)
def build_dcn(input_dim,
              cross_layers=5,
              deep_units: list = None,
              dropout_rate: float = 0.5,
              learning_rate: float = None,
              use_batch_norm=False
              ):
    """
    æ„å»ºæ·±åº¦äº¤å‰ç½‘ç»œï¼ˆDeep & Cross Network, DCNï¼‰æ¨¡å‹ã€‚

    :param input_dim: è¾“å…¥ç‰¹å¾çš„ç»´åº¦ã€‚
    :param cross_layers: äº¤å‰å±‚çš„æ•°é‡ï¼Œé»˜è®¤ä¸º5å±‚ã€‚
            å°æ ·æœ¬ï¼ˆå‡ åƒï¼‰+ é«˜ç»´ï¼ˆ>200ç»´ï¼‰	    1 ~ 2 å±‚ï¼ˆè¿‡å¤šå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
            ä¸­æ ·æœ¬ï¼ˆå‡ ä¸‡ï¼‰+ ä¸­ç»´ï¼ˆå‡ å ~ ç™¾ï¼‰	2 ~ 3 å±‚ï¼ˆå…¸å‹è®¾ç½®ï¼‰
            å¤§æ ·æœ¬ï¼ˆç™¾ä¸‡ï¼‰+ ä½ç»´ï¼ˆ<50ï¼‰	        3 ~ 4 å±‚å¯å°è¯•ï¼ˆæå‡é«˜é˜¶è¡¨è¾¾ï¼‰
    :param deep_units: æ·±åº¦éƒ¨åˆ†æ¯å±‚çš„å•å…ƒæ•°åˆ—è¡¨ï¼Œé»˜è®¤ä¸º[128, 64, 32]ã€‚
            å°æ ·æœ¬ï¼ˆå‡ åƒï¼‰+ é«˜ç»´ï¼ˆ>200ç»´ï¼‰	    [64, 32] æˆ– [128, 64, 32]
            ä¸­æ ·æœ¬ï¼ˆå‡ ä¸‡ï¼‰	                [128, 64, 32] æˆ– [256, 128, 64]
            å¤§æ ·æœ¬ï¼ˆç™¾ä¸‡ï¼‰	                [512, 256, 128] æˆ–æ›´æ·±
    :param dropout_rate: Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé»˜è®¤ä¸º0.5ã€‚
            å°æ ·æœ¬ï¼ˆå‡ åƒï¼‰     0.4 ~ 0.6
            ä¸­æ ·æœ¬ï¼ˆå‡ ä¸‡ï¼‰     0.3 ~ 0.5
            å¤§æ ·æœ¬ï¼ˆç™¾ä¸‡ï¼‰     0.2 ~ 0.4
    :param learning_rate: å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º None è¡¨ç¤ºä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡ã€‚
            å°æ ·æœ¬ + å°ç½‘ç»œ	    0.01 ~ 0.001
            ä¸­ç­‰æ ·æœ¬ + ä¸­æ·±ç½‘ç»œ	0.001 ~ 0.0005ï¼ˆæ¨èï¼‰
            å¤§æ ·æœ¬ + æ·±æ¨¡å‹	    0.0005 ~ 0.0001
    :param use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰ï¼Œé»˜è®¤ä¸ºFalseã€‚
            æ·±å±‚ç½‘ç»œï¼ˆ3å±‚ä»¥ä¸Šï¼‰	    âœ… æ¨è
            ç‰¹å¾åˆ†å¸ƒå·®å¼‚å¤§ï¼ˆå¦‚é™ç»´åï¼‰	âœ… æ¨è
            å°æ ·æœ¬+è¿‡æ‹Ÿåˆæ˜æ˜¾	        âŒ å¯å…ˆä¸ç”¨ï¼Œå…ˆç”¨ Dropout
    :return: æ„å»ºå¥½çš„DCNæ¨¡å‹ã€‚
    """
    # å®šä¹‰æ¨¡å‹è¾“å…¥
    inputs = Input(shape=(input_dim,))
    x0 = inputs
    xl = inputs

    # å¦‚æœæœªæä¾›deep_unitsï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
    if deep_units is None:
        deep_units = [128, 64, 32]

    # Cross Part ğŸ‘‰ æ˜¾å¼å»ºæ¨¡ç‰¹å¾ä¹‹é—´çš„äº¤å‰ç»„åˆé¡¹
    for _ in range(cross_layers):
        xl_w = Dense(input_dim)(xl)
        cross = Multiply()([x0, xl_w])
        xl = Add()([cross, xl])

    # Deep Part ğŸ‘‰ æ•æ‰é«˜é˜¶ç‰¹å¾ç»„åˆçš„éçº¿æ€§å…³ç³» (å¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œ MLP)
    deep = inputs
    for units in deep_units:
        deep = Dense(units, activation='relu')(deep)
        # æ ¹æ®use_batch_normå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–
        if use_batch_norm:
            from tensorflow.keras.layers import BatchNormalization
            deep = BatchNormalization()(deep)
        # æ ¹æ®dropout_rateå‚æ•°å†³å®šæ˜¯å¦åº”ç”¨dropout
        if dropout_rate > 0:
            from tensorflow.keras.layers import Dropout
            deep = Dropout(dropout_rate)(deep)

    # Final
    concat = Concatenate()([xl, deep])
    output = Dense(1, activation='sigmoid')(concat)

    # æ„å»ºæ¨¡å‹
    if learning_rate is None:  # å¦‚æœæ²¡æœ‰æä¾›å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„åŠ¨æ€å­¦ä¹ ç‡
        lr_schedule = ExponentialDecay(
            initial_learning_rate=0.0001,
            decay_steps=10000,
            decay_rate=0.1,
            staircase=True
        )
        optimizer = Adam(learning_rate=lr_schedule)
    else:  # å¦‚æœæä¾›äº†å­¦ä¹ ç‡ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„å­¦ä¹ ç‡
        optimizer = Adam(learning_rate=learning_rate)

    # å®šä¹‰æ¨¡å‹å¹¶ç¼–è¯‘
    model = Model(inputs=inputs, outputs=output)
    model.compile(
        loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )
    return model


# # Optunaè¯•éªŒçš„ç›®æ ‡å‡½æ•°, ç”¨äºä¼˜åŒ–DCNæ¨¡å‹çš„è¶…å‚æ•°
# def objective_dcn(trial):
#     """
#     Optunaè¯•éªŒçš„ç›®æ ‡å‡½æ•°ï¼Œç”¨äºä¼˜åŒ–DCNæ¨¡å‹çš„è¶…å‚æ•°ã€‚
#
#     å‚æ•°:
#     - trial: Optunaçš„è¯•éªŒå¯¹è±¡ï¼Œç”¨äºé€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
#
#     è¿”å›:
#     - è¿”å›æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„F1åˆ†æ•°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
#     """
#
#     ''' å®šä¹‰æ¨¡å‹çš„è¶…å‚æ•°æœç´¢ç©ºé—´ '''
#     # å»ºè®®äº¤å‰å±‚çš„æ•°é‡ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦
#     cross_layers = trial.suggest_int('cross_layers', 2, 6)
#
#     # å»ºè®®dropoutç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
#     dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.7)
#
#     # å»ºè®®å­¦ä¹ ç‡ï¼Œé‡‡ç”¨å¯¹æ•°åˆ»åº¦ä»¥æ¢ç´¢å¹¿æ³›çš„å¯èƒ½æ€§
#     learning_rate = trial.suggest_float('learning_rate', 0.00001, 0.001, log=True)
#
#     # å»ºè®®æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
#     use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])
#
#     # å»ºè®®æ·±åº¦ç½‘ç»œçš„å•å…ƒé…ç½®ï¼Œé€‰æ‹©ä¸åŒçš„ç½‘ç»œç»“æ„ä»¥å¯»æ‰¾æœ€ä¼˜è§£
#     deep_units = trial.suggest_categorical('deep_units', [
#         [128, 64],
#         [128, 64, 32],
#         [128, 64, 32, 16],
#         [64, 32],
#         [64, 32, 16],
#         [64, 32, 16, 8],
#         [32, 16],
#         [32, 16, 8],
#     ])
#
#     ''' æ„å»ºå…·æœ‰é€‰å®šè¶…å‚æ•°çš„DCNæ¨¡å‹ '''
#     model = build_dcn(
#         input_dim=x_train.shape[1],
#         cross_layers=cross_layers,
#         deep_units=deep_units,
#         dropout_rate=dropout_rate,
#         learning_rate=learning_rate,
#         use_batch_norm=use_batch_norm
#     )
#
#     # è®­ç»ƒæ¨¡å‹
#     model.fit(
#         x_train, y_train,  # è¾“å…¥è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾
#         epochs=15,  # è®¾ç½®è®­ç»ƒçš„è½®æ•°ä¸º15è½®
#         batch_size=32,  # æŒ‡å®šæ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°ä¸º32ä¸ªæ ·æœ¬
#         validation_split=0.1,  # å°†10%çš„è®­ç»ƒæ•°æ®ç”¨ä½œéªŒè¯æ•°æ®
#         verbose=0  # è®¾ç½®è®­ç»ƒè¿‡ç¨‹ä¸è¾“å‡ºæ—¥å¿—ä¿¡æ¯
#     )
#
#     ''' ä½¿ç”¨F1åˆ†æ•°è¯„ä¼°æ¨¡å‹æ€§èƒ½ '''
#     y_pred = (model.predict(x_test) > 0.5).astype(int)
#     return f1_score(y_test, y_pred)


def objective_dcn(trial, validation_mode='kfold', n_splits=5):
    """
    Optunaè¯•éªŒçš„ç›®æ ‡å‡½æ•°ï¼Œç”¨äºä¼˜åŒ–DCNæ¨¡å‹çš„è¶…å‚æ•°ã€‚

    å‚æ•°:
    - trial: Optunaçš„è¯•éªŒå¯¹è±¡ï¼Œç”¨äºé€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
    - validation_mode: é€‰æ‹© 'split' æˆ– 'kfold'ï¼Œç”¨äºé€‰æ‹©éªŒè¯æ–¹å¼ã€‚

    è¿”å›:
    - å¹³å‡F1åˆ†æ•°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    """

    # è¶…å‚æ•°ç©ºé—´å®šä¹‰
    cross_layers = trial.suggest_int('cross_layers', 2, 6)
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.7)
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])
    deep_units = trial.suggest_categorical('deep_units', [
        [128, 64],
        [128, 64, 32],
        [128, 64, 32, 16],
        [64, 32],
        [64, 32, 16],
        [64, 32, 16, 8],
        [32, 16],
        [32, 16, 8],
    ])

    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)

    # æ¨¡å‹è®­ç»ƒä¸éªŒè¯
    if validation_mode == 'split':
        # å›ºå®šåˆ’åˆ†éªŒè¯é›†
        x_subtrain, x_val, y_subtrain, y_val = train_test_split(
            x_train, y_train, test_size=0.1, shuffle=False
        )

        model = build_dcn(
            input_dim=x_train.shape[1],
            cross_layers=cross_layers,
            deep_units=deep_units,
            dropout_rate=dropout_rate,
            learning_rate=learning_rate,
            use_batch_norm=use_batch_norm
        )

        model.fit(
            x_subtrain, y_subtrain,
            validation_data=(x_val, y_val),
            epochs=30,
            batch_size=32,
            verbose=0,
            callbacks=[early_stop]
        )

        y_val_pred = (model.predict(x_val) > 0.5).astype(int)
        return f1_score(y_val, y_val_pred)

    elif validation_mode == 'kfold':
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
        f1_scores = []

        for train_idx, val_idx in kf.split(x_train):
            x_subtrain, x_val = x_train[train_idx], x_train[val_idx]
            y_subtrain, y_val = y_train[train_idx], y_train[val_idx]

            model = build_dcn(
                input_dim=x_train.shape[1],
                cross_layers=cross_layers,
                deep_units=deep_units,
                dropout_rate=dropout_rate,
                learning_rate=learning_rate,
                use_batch_norm=use_batch_norm
            )

            model.fit(
                x_subtrain, y_subtrain,
                validation_data=(x_val, y_val),
                epochs=30,
                batch_size=32,
                verbose=0,
                callbacks=[early_stop]
            )

            y_val_pred = (model.predict(x_val) > 0.5).astype(int)
            f1 = f1_score(y_val, y_val_pred)
            f1_scores.append(f1)

        return np.mean(f1_scores)

    else:
        raise ValueError("validation_mode must be either 'split' or 'kfold'")


# è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
def train_and_evaluate(model, x_train, y_train, x_test, y_test, epochs=10, batch_size=32):
    """
    è®­ç»ƒå¹¶è¯„ä¼°ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

    å‚æ•°:
    - model: è¦è®­ç»ƒå’Œè¯„ä¼°çš„æ¨¡å‹ã€‚
    - x_train: è®­ç»ƒæ•°æ®ç‰¹å¾ã€‚
    - y_train: è®­ç»ƒæ•°æ®æ ‡ç­¾ã€‚
    - x_test: æµ‹è¯•æ•°æ®ç‰¹å¾ã€‚
    - y_test: æµ‹è¯•æ•°æ®æ ‡ç­¾ã€‚
    - epochs: è®­ç»ƒçš„è½®æ•°ï¼Œé»˜è®¤ä¸º10ã€‚
    - batch_size: æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡çš„å¤§å°ï¼Œé»˜è®¤ä¸º32ã€‚

    æ­¤å‡½æ•°æ²¡æœ‰è¿”å›å€¼ï¼Œä½†ä¼šåœ¨æ§åˆ¶å°è¾“å‡ºæ¨¡å‹çš„è¯„ä¼°æŠ¥å‘Šã€‚
    """
    # è®­ç»ƒæ¨¡å‹
    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2, validation_split=0.1)

    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹å¹¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶åˆ†ç±»
    pred = (model.predict(x_test) > 0.5).astype(int)

    # è¾“å‡ºåˆ†ç±»æŠ¥å‘Š
    print(classification_report(y_test, pred))


if __name__ == '__main__':
    from ReturnClassification.train_and_test import main_data_prepare

    # æ•°æ®è¯»å–ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä»¥åŠ åŸå§‹æŒ‡æ ‡æ•°æ®
    x_train, y_train, x_test, y_test, metrics_data = main_data_prepare(
        the_fund_code='510050.SH',  # æŒ‡å®šåŸºé‡‘ä»£ç ï¼Œæ­¤å¤„ä¸º '510050.SH'
        n_days=10,
        folder_path='../Data',  # åŸºé‡‘ä»·æ ¼æ•°æ®çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º '../Data'
        metrics_folder='../Data/Metrics',  # åŸºé‡‘æŒ‡æ ‡æ•°æ®çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º '../Data/Metrics'
        train_start=None,  # è®­ç»ƒé›†å¼€å§‹æ—¥æœŸï¼Œå¦‚æœä¸º Noneï¼Œåˆ™ä»æ•°æ®çš„æœ€æ—©æ—¥æœŸå¼€å§‹
        train_end='2024-11-30',  # è®­ç»ƒé›†ç»“æŸæ—¥æœŸï¼ŒæŒ‡å®šä¸º '2024-11-30'
        test_start='2024-12-01',  # æµ‹è¯•é›†å¼€å§‹æ—¥æœŸï¼ŒæŒ‡å®šä¸º '2024-12-01'
        test_end='2025-04-30',  # æµ‹è¯•é›†ç»“æŸæ—¥æœŸï¼ŒæŒ‡å®šä¸º '2025-04-30'
        nan_method='drop',  # å¤„ç†ç¼ºå¤±å€¼çš„æ–¹æ³•ï¼Œé»˜è®¤ä¸º 'drop'ï¼ˆåˆ é™¤ç¼ºå¤±å€¼ï¼‰ï¼Œå¯é€‰ 'median' æˆ– 'mean'
        standardize_method='zscore',  # æŒ‡æ ‡æ ‡å‡†åŒ–çš„æ–¹æ³•,å¯é€‰: 'minmax', 'zscore', 'both', 'none'ã€‚
        basic_data_as_metric=True,  # æ˜¯å¦å°†åŸºæœ¬æ•°æ®ï¼ˆå¦‚å¼€ç›˜ä»·ã€æ”¶ç›˜ä»·ã€äº¤æ˜“é‡ç­‰ï¼‰ä½œä¸ºç‰¹å¾æ•°æ®ï¼Œé»˜è®¤ä¸º True
        return_threshold=0.0,  # æ ‡ç­¾ç”Ÿæˆæ–¹æ³•ï¼Œæœªæ¥æ”¶ç›Šç‡å¤§äº 0.01 çš„æ ·æœ¬æ ‡è®°ä¸º 1ï¼Œå¦åˆ™ä¸º 0
        dim_reduction=True,  # æ˜¯å¦è¿›è¡Œç‰¹å¾é™ç»´ï¼Œé»˜è®¤ä¸º False
        n_components=None,  # PCAé™ç»´çš„ç›®æ ‡ç»´åº¦ï¼Œé»˜è®¤ä¸º 100
    )

    # æ•°æ®é¢„å¤„ç†
    # æ•°æ®é¢„å¤„ç†ï¼ˆæ³¨æ„ç»Ÿä¸€scalerï¼‰
    scaler = MinMaxScaler()
    x_train_scaled = scaler.fit_transform(x_train)
    x_test_scaled = scaler.transform(x_test)
    y_train_values = y_train.values
    y_test_values = y_test.values

    input_dim = x_train.shape[1]

    ''' 1) DeepFM æ¨¡å‹ '''
    print(" --- " * 20)
    print("Training DeepFM...")
    embed_dim = 8
    deep_fm_model = build_deep_fm(input_dim, embed_dim)
    train_and_evaluate(deep_fm_model, x_train_scaled, y_train_values, x_test_scaled, y_test_values)

    # ''' 2) Wide & Deep æ¨¡å‹ '''
    # print(" --- " * 20)
    # print("\nTraining Wide & Deep...")
    # # ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
    # study = optuna.create_study(direction='maximize')
    # study.optimize(objective_wide_deep,
    #                n_trials=150  # è®¾ç½®è¯•éªŒæ¬¡æ•°
    #                )
    # print("Best trial:", study.best_trial.params)  # è¾“å‡ºæœ€ä¼˜è¶…å‚æ•°
    # # Best trial: {'dropout_rate': 0.3611156413233256, 'use_batch_norm': True, 'wide_activation': 'sigmoid', 'activation': 'relu', 'learning_rate': 0.0015255087830830473, 'deep_units': [128, 64]}
    #
    # # ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    # wd_model = build_wide_deep(
    #     input_dim=input_dim,  # è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼Œå³æ¨¡å‹è¾“å…¥çš„å¤§å°ã€‚
    #     deep_units=study.best_trial.params['deep_units'],
    #     dropout_rate=study.best_trial.params['dropout_rate'],
    #     use_batch_norm=study.best_trial.params['use_batch_norm'],
    #     wide_activation=study.best_trial.params['wide_activation'],
    #     activation=study.best_trial.params['activation'],
    #     learning_rate=study.best_trial.params['learning_rate'],
    # )
    # # è®­ç»ƒå¹¶è¯„ä¼°Wide&Deepæ¨¡å‹
    # train_and_evaluate(wd_model,  # è¦è®­ç»ƒå’Œè¯„ä¼°çš„Wide&Deepæ¨¡å‹ï¼Œç”± `build_wide_deep` å‡½æ•°æ„å»ºï¼Œä½¿ç”¨äº† Optuna ä¼˜åŒ–åçš„è¶…å‚æ•°ã€‚
    #                    x_train_scaled,  # è®­ç»ƒæ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†ï¼ˆå¦‚å½’ä¸€åŒ–ï¼‰åçš„ç‰¹å¾çŸ©é˜µï¼Œç¡®ä¿è¾“å…¥æ•°æ®å…·æœ‰ç›¸åŒçš„å°ºåº¦ã€‚
    #                    y_train_values,  # è®­ç»ƒæ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºè®­ç»ƒç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºç›‘ç£å­¦ä¹ è¿‡ç¨‹ã€‚
    #                    x_test_scaled,  # æµ‹è¯•æ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†åçš„ç‰¹å¾çŸ©é˜µï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
    #                    y_test_values,  # æµ‹è¯•æ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºæµ‹è¯•ç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æŒ‡æ ‡ã€‚
    #                    epochs=10,  # æŒ‡å®šæ¨¡å‹è®­ç»ƒçš„è½®æ•°ï¼ˆepochï¼‰ï¼Œå³æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šå®Œæ•´è®­ç»ƒçš„æ¬¡æ•°ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
    #                    batch_size=32  # æŒ‡å®šæ¯ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰çš„å¤§å°ï¼Œå³æ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
    #                    )

    # ''' 3) NCD æ¨¡å‹ '''
    # print(" --- " * 20)
    # print("\nTraining DCN...")
    # # ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
    # study = optuna.create_study(direction='maximize')
    # study.optimize(
    #     lambda trial: objective_dcn(trial, validation_mode='kfold'),
    #     n_trials=30
    # )
    # print("Best trial:", study.best_trial.params)  # è¾“å‡ºæœ€ä¼˜è¶…å‚æ•°
    # # Best trial: {'cross_layers': 2, 'dropout_rate': 0.2043031648581299, 'learning_rate': 0.0009367655839776305, 'use_batch_norm': False, 'deep_units': [128, 64]}
    #
    # # ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    # dcn_model = build_dcn(
    #     input_dim=input_dim,  # è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼Œå³æ¨¡å‹è¾“å…¥çš„å¤§å°ã€‚
    #     cross_layers=study.best_trial.params['cross_layers'],  # äº¤å‰å±‚çš„æ•°é‡ï¼Œæ§åˆ¶æ¨¡å‹ä¸­æ˜¾å¼å»ºæ¨¡ç‰¹å¾äº¤å‰ç»„åˆçš„å±‚æ•°ã€‚
    #     deep_units=study.best_trial.params['deep_units'],  # æ·±åº¦éƒ¨åˆ†æ¯å±‚çš„å•å…ƒæ•°åˆ—è¡¨ï¼Œå®šä¹‰æ·±åº¦ç½‘ç»œçš„ç»“æ„ã€‚
    #     dropout_rate=study.best_trial.params['dropout_rate'],  # Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œéšæœºä¸¢å¼ƒç¥ç»å…ƒçš„æ¯”ä¾‹ã€‚
    #     learning_rate=study.best_trial.params['learning_rate'],  # å­¦ä¹ ç‡ï¼Œæ§åˆ¶ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°çš„é€Ÿåº¦ã€‚
    #     use_batch_norm=study.best_trial.params['use_batch_norm'],  # æ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒå’Œæé«˜æ¨¡å‹ç¨³å®šæ€§ã€‚
    # )
    #
    # # è®­ç»ƒå¹¶è¯„ä¼°DCNæ¨¡å‹
    # train_and_evaluate(
    #     dcn_model,  # è¦è®­ç»ƒå’Œè¯„ä¼°çš„DCNæ¨¡å‹ï¼Œç”± `build_dcn` å‡½æ•°æ„å»ºï¼Œä½¿ç”¨äº† Optuna ä¼˜åŒ–åçš„è¶…å‚æ•°ã€‚
    #     x_train_scaled,  # è®­ç»ƒæ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†ï¼ˆå¦‚å½’ä¸€åŒ–ï¼‰åçš„ç‰¹å¾çŸ©é˜µï¼Œç¡®ä¿è¾“å…¥æ•°æ®å…·æœ‰ç›¸åŒçš„å°ºåº¦ã€‚
    #     y_train_values,  # è®­ç»ƒæ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºè®­ç»ƒç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºç›‘ç£å­¦ä¹ è¿‡ç¨‹ã€‚
    #     x_test_scaled,  # æµ‹è¯•æ•°æ®ç‰¹å¾ï¼Œç»è¿‡é¢„å¤„ç†åçš„ç‰¹å¾çŸ©é˜µï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
    #     y_test_values,  # æµ‹è¯•æ•°æ®æ ‡ç­¾ï¼Œå¯¹åº”äºæµ‹è¯•ç‰¹å¾çš„çœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æŒ‡æ ‡ã€‚
    #     epochs=10,  # æŒ‡å®šæ¨¡å‹è®­ç»ƒçš„è½®æ•°ï¼ˆepochï¼‰ï¼Œå³æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šå®Œæ•´è®­ç»ƒçš„æ¬¡æ•°ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
    #     batch_size=32  # æŒ‡å®šæ¯ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰çš„å¤§å°ï¼Œå³æ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ã€‚è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ, è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚
    # )
