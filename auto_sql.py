import json
import traceback
import os
import httpx
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from openai import OpenAI
from bs4 import BeautifulSoup
import time

# --- é…ç½®ä¸­å¿ƒ ---
CONFIG = {
    "login_url": "https://dd.gildata.com/#/login",
    "table_to_search": "SecuMain",
    "api_key": "xxxx",
    "base_url": "https://api.deepseek.com",
    "model_name": "deepseek-chat",
    "window_size": (1920, 1080),
    "explicit_wait_timeout": 20,
    "final_observe_time": 5,
    "element_cache_file": "element_cache.json",
    "output_json_file": "table_definitions.json",
}


# --- è®°å¿†/ç¼“å­˜/æ•°æ®åº“ åŠŸèƒ½ ---
def load_json_file(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}
    return {}


def save_json_file(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)


# --- æµè§ˆå™¨ä¸å…ƒç´ æ“ä½œ ---
def launch_browser():
    options = webdriver.ChromeOptions()
    if CONFIG.get("window_size"):
        width, height = CONFIG["window_size"]
        options.add_argument(f"--window-size={width},{height}")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


def find_element_safely(driver, by, value, timeout=CONFIG["explicit_wait_timeout"]):
    try:
        wait = WebDriverWait(driver, timeout)
        by_obj = getattr(By, by.upper())
        return wait.until(EC.presence_of_element_located((by_obj, value)))
    except Exception:
        return None


# --- æ•°æ®æå–ä¸AIæ•´ç†æ¨¡å— ---
def simplify_html(html_source):
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'link', 'meta']):
        tag.decompose()
    body = soup.find('body')
    if body:
        return str(body.prettify())
    return ""


def get_locator_from_ai(html_source, element_description):
    print("ğŸ¤– æ­£åœ¨ç²¾ç®€HTMLä»¥é€‚åº”æ¨¡å‹ä¸Šä¸‹æ–‡...")
    simplified_html = simplify_html(html_source)
    if len(simplified_html) > 200000:
        print(f"âŒ ç²¾ç®€åçš„HTMLä»ç„¶è¿‡é•¿ ({len(simplified_html)} chars)ï¼Œè·³è¿‡APIè°ƒç”¨ã€‚")
        return None

    print("ğŸ¤– æ­£å¼è°ƒç”¨å¤§æ¨¡å‹APIè¿›è¡Œåˆ†æ...")
    try:
        http_client = httpx.Client(verify=False)
        client = OpenAI(api_key=CONFIG["api_key"], base_url=CONFIG["base_url"], http_client=http_client)
        system_prompt = (
            "You are an expert web automation assistant. Your task is to analyze HTML source code "
            "and return a single, precise, and robust Selenium locator for a requested element. "
            "You must return the result as a JSON object with two keys: 'by' and 'value'. "
            "The 'by' key must be one of the following strings: 'ID', 'NAME', 'CLASS_NAME', 'TAG_NAME', 'LINK_TEXT', 'PARTIAL_LINK_TEXT', 'CSS_SELECTOR', 'XPATH'. "
            "The 'value' key is the corresponding locator string."
        )
        user_prompt = (
            f"Based on the following HTML, find the locator for the '{element_description}'.\n\n"
            f"HTML:\n```html\n{simplified_html}\n```\n\n"
            "Return only the JSON object."
        )
        response = client.chat.completions.create(
            model=CONFIG["model_name"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            stream=False
        )
        response_text = response.choices[0].message.content
        print(f"ğŸ¤– å¤§æ¨¡å‹è¿”å›åŸå§‹ç»“æœ: {response_text}")
        json_part = response_text[response_text.find('{'):response_text.rfind('}') + 1]
        locator = json.loads(json_part)
        if isinstance(locator, dict) and 'by' in locator and 'value' in locator:
            return locator
        else:
            print("âŒ å¤§æ¨¡å‹è¿”å›çš„JSONæ ¼å¼ä¸æ­£ç¡®ã€‚")
            return None
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(traceback.format_exc())
        return None


def scrape_table_details(driver):
    """ä»è¡¨è¯¦æƒ…é¡µHTMLä¸­æŠ“å–æ‰€æœ‰ä¿¡æ¯(v14.0 Seleniumç›´æ¥æå–ç‰ˆ)"""
    print("ğŸ” æ­£åœ¨ä½¿ç”¨Seleniumç›´æ¥æå–é¡µé¢å…ƒç´ ...")
    scraped_data = {"basic_info": {}, "columns_data": [], "notes_map": {}}

    # 1. æå–åŸºæœ¬ä¿¡æ¯ (ä½¿ç”¨Seleniumç›´æ¥æå–)
    basic_info_labels = {
        'è¡¨ä¸­æ–‡å': 'tableChineseName',
        'çŠ¶æ€': 'status'
    }
    for label_text, key_name in basic_info_labels.items():
        try:
            # æŸ¥æ‰¾åŒ…å«æ ‡ç­¾æ–‡æœ¬çš„å…ƒç´ ï¼Œç„¶åè·å–å…¶å…„å¼ŸèŠ‚ç‚¹
            label_element = driver.find_element(By.XPATH,
                                                f"//span[contains(text(), '{label_text}')] | //div[contains(text(), '{label_text}')] | //dt[contains(text(), '{label_text}')] ")
            value_element = label_element.find_element(By.XPATH, "following-sibling::*")
            scraped_data["basic_info"][key_name] = value_element.text.strip()
        except Exception:
            pass  # å¿½ç•¥æ‰¾ä¸åˆ°çš„æ ‡ç­¾

    # 2. æå–åˆ—ä¿¡æ¯ (ä½¿ç”¨Seleniumç›´æ¥è·å–outerHTML)
    try:
        column_table_element = driver.find_element(By.CSS_SELECTOR, 'table.table-column.table-interval-bg')
        column_table_html = driver.execute_script("return arguments[0].outerHTML;", column_table_element)
        soup_column_table = BeautifulSoup(column_table_html, 'html.parser')

        headers = [th.text.strip() for th in soup_column_table.find('thead').find_all('th')]
        rows = soup_column_table.find('tbody').find_all('tr', attrs={'ng-repeat': re.compile(r'column in columns')})

        for row in rows:
            cells = row.find_all('td')
            col_dict = {}
            for i, header in enumerate(headers):
                if i < len(cells):
                    col_dict[header] = cells[i].text.strip()
            scraped_data["columns_data"].append(col_dict)
    except Exception as e:
        print(f"âš ï¸ æœªèƒ½ä½¿ç”¨ Selenium å®šä½åˆ°åˆ—è¡¨æ ¼æˆ–æå–åˆ—æ•°æ®ã€‚é”™è¯¯: {e}")

    # 3. æå–å¤‡æ³¨ä¿¡æ¯ (ä½¿ç”¨Seleniumç›´æ¥è·å–outerHTML)
    try:
        remark_table_element = driver.find_element(By.CSS_SELECTOR, 'table.table-remark')
        remark_table_html = driver.execute_script("return arguments[0].outerHTML;", remark_table_element)
        soup_remark_table = BeautifulSoup(remark_table_html, 'html.parser')

        rows = soup_remark_table.find('tbody').find_all('tr')
        for row in rows:
            key_cell = None
            value_cell = None
            try:
                key_cell = row.find_element(By.CSS_SELECTOR, 'td.center')
                value_cell = row.find_element(By.CSS_SELECTOR, 'td:not(.center)')  # æŸ¥æ‰¾écenterç±»çš„td
            except Exception:
                pass  # å¿½ç•¥æ‰¾ä¸åˆ°çš„å•å…ƒæ ¼

            if key_cell and value_cell:
                key = key_cell.text.strip().replace('[', '').replace(']', '').strip()
                value = value_cell.text.strip()
                scraped_data["notes_map"][key] = value
    except Exception as e:
        print(f"âš ï¸ æœªèƒ½ä½¿ç”¨ Selenium å®šä½åˆ°å¤‡æ³¨è¡¨æ ¼æˆ–æå–å¤‡æ³¨æ•°æ®ã€‚é”™è¯¯: {e}")

    print(f"âœ… æŠ“å–å®Œæˆï¼šæ‰¾åˆ° {len(scraped_data['columns_data'])} åˆ—æ•°æ®ï¼Œ{len(scraped_data['notes_map'])} æ¡å¤‡æ³¨ã€‚")
    return scraped_data


def organize_data_with_ai(table_name, scraped_data):
    print("ğŸ¤– æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œæ•°æ®æ•´åˆä¸æ¸…æ´—...")
    try:
        http_client = httpx.Client(verify=False)
        client = OpenAI(api_key=CONFIG["api_key"], base_url=CONFIG["base_url"], http_client=http_client)
        system_prompt = (
            "You are a data structuring expert. Your job is to assemble raw, pre-scraped data parts into a final, clean JSON object representing a database table definition."
        )
        user_prompt = (
            f"Please assemble the following data for the table named '{table_name}'.\n\n"
            f"1. Basic table information:\n"
            f"```json\n{json.dumps(scraped_data['basic_info'], indent=2, ensure_ascii=False)}\n```\n\n"
            f"2. A list of table columns:\n"
            f"```json\n{json.dumps(scraped_data['columns_data'], indent=2, ensure_ascii=False)}\n```\n\n"
            f"3. A mapping of note keys to their full explanatory text:\n"
            f"```json\n{json.dumps(scraped_data['notes_map'], indent=2, ensure_ascii=False)}\n```\n\n"
            "Your tasks:\n"
            "- Create a single JSON object.\n"
            "- The root object must have a 'tableName' key with the value '{table_name}'.\n"
            "- Merge the 'basic_info' into the root object.\n"
            "- Add a 'columns' key to the root, containing the list of columns.\n"
            "- For each column in the 'columns' array, if its 'å¤‡æ³¨' value (e.g., 'æ³¨1') is a key in the notes mapping, replace that value with the full text from the mapping.\n"
            "- Return ONLY the final, assembled JSON object and nothing else."
        )
        response = client.chat.completions.create(
            model=CONFIG["model_name"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            stream=False
        )
        response_text = response.choices[0].message.content
        json_part = response_text[response_text.find('{'):response_text.rfind('}') + 1]
        return json.loads(json_part)
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œæ•°æ®æ•´ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


# --- æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ ---
def process_table_details_page(driver, table_name):
    """å¤„ç†è¡¨è¯¦æƒ…é¡µï¼Œæå–ã€æ•´ç†å¹¶ä¿å­˜æ•°æ®"""
    print("--- å¯¼èˆªåˆ°è¯¦æƒ…é¡µæˆåŠŸï¼Œå¼€å§‹æ•°æ®æå–æµç¨‹ --- ")
    # å¢åŠ ä¸€ä¸ªçŸ­æš‚çš„å›ºå®šç­‰å¾…ï¼Œç»™JSåˆå§‹åŒ–ç•™å‡ºæ—¶é—´
    time.sleep(3)

    try:
        wait = WebDriverWait(driver, CONFIG["explicit_wait_timeout"])
        # ç­‰å¾…è¡¨æ ¼å†…éƒ¨è‡³å°‘ä¸€ä¸ªtrå‡ºç°å³å¯ï¼Œæ”¾å®½å¯¹ng-repeatçš„ä¸¥æ ¼è¦æ±‚
        table_row_locator = (By.CSS_SELECTOR, "table.table-column.table-interval-bg tbody tr")
        print(f"â³ æ­£åœ¨ç­‰å¾…å…ƒç´ å‡ºç°: {table_row_locator}")
        wait.until(EC.presence_of_element_located(table_row_locator))
        print("--- åˆ—è¡¨æ ¼å†…å®¹å·²åŠ è½½ï¼Œå¼€å§‹æŠ“å–æ•°æ® --- ")
    except Exception as e:
        print(f"âŒ åœ¨ç­‰å¾…æ—¶é—´å†…æœªèƒ½æ‰¾åˆ°åˆ—è¡¨æ ¼å†…å®¹ ({table_row_locator})ã€‚é¡µé¢å¯èƒ½åŠ è½½å¤±è´¥æˆ–ç»“æ„å·²æ”¹å˜ã€‚é”™è¯¯: {e}")
        # è°ƒè¯•æ­¥éª¤ï¼šä¿å­˜å½“å‰é¡µé¢HTMLï¼Œä»¥ä¾¿åˆ†æ
        debug_file = "debug_page_source.html"
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"â„¹ï¸ ä¸ºäº†ä¾¿äºè°ƒè¯•ï¼Œå½“å‰çš„é¡µé¢HTMLå·²ä¿å­˜åˆ°: {os.path.abspath(debug_file)}")
        return

    # ç›´æ¥å°†driverä¼ é€’ç»™scrape_table_detailsï¼Œç”±å…¶ç›´æ¥ä»DOMä¸­æå–æ•°æ®
    scraped_data = scrape_table_details(driver)
    if not scraped_data or not scraped_data["columns_data"]:
        print("âŒ æœªèƒ½ä»é¡µé¢æŠ“å–åˆ°åˆ—ä¿¡æ¯ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return

    final_table_json = organize_data_with_ai(table_name, scraped_data)
    if final_table_json:
        print("ğŸ’¾ æ­£åœ¨ä»¥å¢é‡/æ›´æ–°æ¨¡å¼ä¿å­˜æ•°æ®...")
        database = load_json_file(CONFIG["output_json_file"])
        database[table_name] = final_table_json
        save_json_file(database, CONFIG["output_json_file"])
        print(f"âœ… æˆåŠŸå°†è¡¨ '{table_name}' çš„å®šä¹‰ä¿å­˜åˆ° {CONFIG['output_json_file']}ã€‚")
    else:
        print("âŒ AIæœªèƒ½æˆåŠŸæ•´ç†æ•°æ®ï¼Œæœ¬æ¬¡ä¸ä¿å­˜ã€‚")


def login_and_search(driver):
    driver.get(CONFIG["login_url"])
    cache = load_json_file(CONFIG["element_cache_file"])
    input("è¯·æ‰‹åŠ¨å®Œæˆç™»å½•åï¼Œå›åˆ°ç»ˆç«¯æŒ‰å›è½¦ç»§ç»­...")
    table_name = CONFIG["table_to_search"]
    print(f"ç™»å½•å®Œæˆï¼Œå¼€å§‹æŸ¥æ‰¾è¡¨ï¼š{table_name}")

    search_box_locator = cache.get("search_box")
    search_box = None
    if search_box_locator:
        print("ğŸ§  æ­£åœ¨å°è¯•ä»è®°å¿†ä¸­å®šä½æœç´¢æ¡†...")
        search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"], timeout=3)

    if not search_box:
        print("ğŸ¤” è®°å¿†å¤±æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œå¯åŠ¨å¤§æ¨¡å‹æ™ºèƒ½å‘ç°æ¨¡å¼...")
        html = driver.page_source
        search_box_locator = get_locator_from_ai(html, "the main search input box for table names")
        if search_box_locator:
            print(f"ğŸ¤– AIå‘ç°æœç´¢æ¡†å®šä½å™¨: {search_box_locator}")
            cache["search_box"] = search_box_locator
            save_json_file(cache, CONFIG["element_cache_file"])
            search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"])
        else:
            print("âŒ æ™ºèƒ½å‘ç°å¤±è´¥ï¼Œæ— æ³•æ‰¾åˆ°æœç´¢æ¡†ã€‚")
            return

    if not search_box:
        print("âŒ æœ€ç»ˆæœªèƒ½å®šä½åˆ°æœç´¢æ¡†ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    print("âœ… æˆåŠŸå®šä½åˆ°æœç´¢æ¡†ã€‚")
    search_box.clear()
    search_box.send_keys(table_name)
    search_box.send_keys(Keys.ENTER)

    print(f"æ­£åœ¨æœç´¢ '{table_name}' çš„é“¾æ¥...")
    try:
        wait = WebDriverWait(driver, CONFIG["explicit_wait_timeout"])
        # å®šä½åˆ°æˆ‘ä»¬æƒ³è¦ç‚¹å‡»çš„é‚£ä¸ªç²¾ç¡®çš„é“¾æ¥
        link_xpath = f"//a[contains(@href, '/tableShow/') and normalize-space(.)='{table_name}']"
        link_element = wait.until(EC.presence_of_element_located((By.XPATH, link_xpath)))

        # --- ç»ˆæè§£å†³æ–¹æ¡ˆï¼šæ”¾å¼ƒæ¨¡æ‹Ÿç‚¹å‡»ï¼Œç›´æ¥æå–hrefå¹¶å¯¼èˆª ---
        target_url = link_element.get_attribute('href')
        print(f"âœ… æˆåŠŸå®šä½é“¾æ¥ï¼Œæå–åˆ°ç›®æ ‡URL: {target_url}")

        if not target_url:
            print("âŒ æ— æ³•ä»é“¾æ¥å…ƒç´ ä¸­æå–åˆ°hrefå±æ€§ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        print("ğŸš€ æ­£åœ¨ç›´æ¥å¯¼èˆªåˆ°ç›®æ ‡URL...")
        driver.get(target_url)

        # ç›´æ¥å¯¼èˆªåï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ç­‰å¾…æ–°è§†å›¾çš„æ ‡å¿—æ€§å…ƒç´ å‡ºç°
        print("â³ æ­£åœ¨ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½...")
        details_page_identifier = (By.CSS_SELECTOR, "table.table-column.table-interval-bg")
        wait.until(EC.presence_of_element_located(details_page_identifier))
        print("âœ… è¯¦æƒ…é¡µåŠ è½½æˆåŠŸã€‚")

        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°è°ƒç”¨è¯¦æƒ…é¡µå¤„ç†å‡½æ•°
        process_table_details_page(driver, table_name)
        print("--- æ•°æ®æå–æµç¨‹ç»“æŸ ---")

    except Exception as e:
        print(f"âš ï¸ åœ¨æŸ¥æ‰¾ã€å¯¼èˆªæˆ–å¤„ç† '{table_name}' æ—¶å¤±è´¥: {e}")
        # ä¿å­˜å¤±è´¥æ—¶çš„é¡µé¢å¿«ç…§ï¼Œä»¥ä¾¿è°ƒè¯•
        debug_file = "debug_page_source_final.html"
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"â„¹ï¸ ä¸ºäº†ä¾¿äºè°ƒè¯•ï¼Œæœ€ç»ˆå¤±è´¥æ—¶çš„é¡µé¢HTMLå·²ä¿å­˜åˆ°: {os.path.abspath(debug_file)}")
        print(traceback.format_exc())


def main():
    driver = None
    try:
        driver = launch_browser()
        login_and_search(driver)
        print(f"\næ“ä½œå®Œæˆï¼Œé¡µé¢å°†ä¿æŒæ‰“å¼€ {CONFIG['final_observe_time']} ç§’...")
        time.sleep(CONFIG['final_observe_time'])
    except Exception as e:
        print(f"ç¨‹åºå‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        print(traceback.format_exc())
    finally:
        if driver:
            print("å…³é—­æµè§ˆå™¨ã€‚")
            driver.quit()


if __name__ == "__main__":
    main()
