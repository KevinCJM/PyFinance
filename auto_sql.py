import json
import traceback
import os
import httpx
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from openai import OpenAI
from bs4 import BeautifulSoup
import time
import random

# --- é…ç½®ä¸­å¿ƒ ---
CONFIG = {
    "login_url": "https://dd.gildata.com/#/login",
    "table_to_search": [
        "HK_SecuMain", "SecuMain", "MF_FundArchives", "MF_FundType", "MF_Transformation",
        "MF_KeyStockPortfolio", "MF_QDIIPortfolioDetail", "MF_BondPortifolioDetail", "MF_QDIIPortfolioDetail",
        "MF_FundPortifolioDetail", "MF_QDIIPortfolioDetail", "MF_BalanceSheetNew", "MF_BondPortifolioStru",
        "MF_AssetAllocationNew", "MF_StockPortfolioDetail", "LC_DIndicesForValuation"
    ],
    "api_key": "sk-a221b0c62c8a460693fe00a627d4598e",
    "base_url": "https://api.deepseek.com",
    "model_name": "deepseek-chat",
    "window_size": (1920, 1080),
    "explicit_wait_timeout": 20,
    "final_observe_time": 5,
    "element_cache_file": "element_cache.json",
    "output_json_file": "table_definitions.json",
    "min_sleep_time": 0.5,
    "max_sleep_time": 2.5,
}


# --- è®°å¿†/ç¼“å­˜/æ•°æ®åº“ åŠŸèƒ½ ---
def load_json_file(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}
    return {}


def save_json_file(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)


def random_sleep():
    sleep_time = random.uniform(CONFIG["min_sleep_time"], CONFIG["max_sleep_time"])
    print(f"ğŸ˜´ ä¼‘çœ  {sleep_time:.2f} ç§’...")
    time.sleep(sleep_time)


# --- æµè§ˆå™¨ä¸å…ƒç´ æ“ä½œ ---
def launch_browser():
    options = webdriver.ChromeOptions()
    if CONFIG.get("window_size"):
        width, height = CONFIG["window_size"]
        options.add_argument(f"--window-size={width},{height}")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


def find_element_safely(driver, by, value, timeout=CONFIG["explicit_wait_timeout"]):
    try:
        wait = WebDriverWait(driver, timeout)
        by_obj = getattr(By, by.upper())
        return wait.until(EC.presence_of_element_located((by_obj, value)))
    except Exception:
        print(traceback.format_exc())
        return None


# --- æ•°æ®æå–ä¸AIæ•´ç†æ¨¡å— ---
def simplify_html(html_source):
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'link', 'meta']):
        tag.decompose()
    body = soup.find('body')
    if body:
        return str(body.prettify())
    return ""


def get_locator_from_ai(html_source, element_description):
    print("ğŸ¤– æ­£åœ¨ç²¾ç®€HTMLä»¥é€‚åº”æ¨¡å‹ä¸Šä¸‹æ–‡...")
    simplified_html = simplify_html(html_source)
    if len(simplified_html) > 200000:
        print(f"âŒ ç²¾ç®€åçš„HTMLä»ç„¶è¿‡é•¿ ({len(simplified_html)} chars)ï¼Œè·³è¿‡APIè°ƒç”¨ã€‚")
        return None

    print("ğŸ¤– æ­£å¼è°ƒç”¨å¤§æ¨¡å‹APIè¿›è¡Œåˆ†æ...")
    try:
        http_client = httpx.Client(verify=False)
        client = OpenAI(api_key=CONFIG["api_key"], base_url=CONFIG["base_url"], http_client=http_client)
        system_prompt = (
            "You are an expert web automation assistant. Your task is to analyze HTML source code "
            "and return a single, precise, and robust Selenium locator for a requested element. "
            "You must return the result as a JSON object with two keys: 'by' and 'value'. "
            "The 'by' key must be one of the following strings: 'ID', 'NAME', 'CLASS_NAME', 'TAG_NAME', 'LINK_TEXT', 'PARTIAL_LINK_TEXT', 'CSS_SELECTOR', 'XPATH'. "
            "The 'value' key is the corresponding locator string."
        )
        user_prompt = (
            f"Based on the following HTML, find the locator for the '{element_description}'.\n\n"
            f"HTML:\n```html\n{simplified_html}\n```\n\n"
            "Return only the JSON object."
        )
        response = client.chat.completions.create(
            model=CONFIG["model_name"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            stream=False
        )
        response_text = response.choices[0].message.content
        print(f"ğŸ¤– å¤§æ¨¡å‹è¿”å›åŸå§‹ç»“æœ: {response_text}")
        json_part = response_text[response_text.find('{'):response_text.rfind('}') + 1]
        locator = json.loads(json_part)
        if isinstance(locator, dict) and 'by' in locator and 'value' in locator:
            return locator
        else:
            print("âŒ å¤§æ¨¡å‹è¿”å›çš„JSONæ ¼å¼ä¸æ­£ç¡®ã€‚")
            return None
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(traceback.format_exc())
        return None


def scrape_table_details(driver):
    """ä»è¡¨è¯¦æƒ…é¡µHTMLä¸­æŠ“å–æ‰€æœ‰ä¿¡æ¯(v14.4 ç»ˆæä¿®æ­£æå–ä¸æ ¼å¼)"""
    print("ğŸ” æ­£åœ¨ä½¿ç”¨Seleniumç›´æ¥æå–é¡µé¢å…ƒç´ ...")
    scraped_data = {"basic_info": {}, "columns_data": [], "notes_map": {}}

    # --- ç»ˆæä¿®æ­£ 1: ä½¿ç”¨æ›´ç²¾ç¡®çš„XPathæå–åŸºæœ¬ä¿¡æ¯ ---
    # å°è¯•æå–â€œè¡¨ä¸­æ–‡åâ€åŠå…¶ä»–åŸºæœ¬ä¿¡æ¯
    try:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        # æŸ¥æ‰¾åŒ…å«ä¸­æ–‡è¡¨åçš„spanæ ‡ç­¾
        chinese_name_span = soup.find('span', {'ng-bind-html': re.compile(r'table\.tableChiName')})
        if chinese_name_span:
            scraped_data["basic_info"]["tableChiName"] = chinese_name_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'è¡¨ä¸­æ–‡å'ã€‚")

        # æå– description
        description_span = soup.find('span', {'ng-bind-html': re.compile(r'table\.description')})
        if description_span:
            scraped_data["basic_info"]["description"] = description_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'description'ã€‚")

        # æå– tableUpdateTime
        table_update_time_span = soup.find('span', {'ng-bind': 'table.tableUpdateTime'})
        if table_update_time_span:
            scraped_data["basic_info"]["tableUpdateTime"] = table_update_time_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'tableUpdateTime'ã€‚")

        # æå– key (ä¸šåŠ¡å”¯ä¸€æ€§)
        key_span = soup.find('span', {'ng-bind': "index.columnName || 'æ— '"})
        if key_span:
            scraped_data["basic_info"]["key"] = key_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'key'ã€‚")

    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æå–åŸºæœ¬ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    # 2. æå–åˆ—ä¿¡æ¯ (é€»è¾‘ä¸å˜)
    try:
        column_table_element = driver.find_element(By.CSS_SELECTOR, 'table.table-column.table-interval-bg')
        column_table_html = driver.execute_script("return arguments[0].outerHTML;", column_table_element)
        soup_column_table = BeautifulSoup(column_table_html, 'html.parser')

        headers = [th.text.strip() for th in soup_column_table.find('thead').find_all('th')]
        # ä¿®æ­£ï¼šng-repeatå¯èƒ½åœ¨tbodyä¸Šï¼Œè€Œä¸æ˜¯trä¸Šï¼ŒæŸ¥æ‰¾æ‰€æœ‰træ›´ç¨³å¦¥
        rows = soup_column_table.find('tbody').find_all('tr')

        for row in rows:
            cells = row.find_all('td')
            if not cells:
                continue  # è·³è¿‡ç©ºè¡Œæˆ–è¡¨å¤´å†…çš„è¡Œ
            col_dict = {}
            for i, header in enumerate(headers):
                if i < len(cells):
                    col_dict[header] = cells[i].text.strip()
            if col_dict:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—å…¸
                scraped_data["columns_data"].append(col_dict)
    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æœªèƒ½ä½¿ç”¨ Selenium å®šä½åˆ°åˆ—è¡¨æ ¼æˆ–æå–åˆ—æ•°æ®ã€‚é”™è¯¯: {e}")

    # --- æ ¸å¿ƒä¿®æ­£ 2: ä¿®æ­£å¤‡æ³¨ä¿¡æ¯æå– ---
    try:
        remark_table_element = driver.find_element(By.CSS_SELECTOR, 'table.table-remark')
        remark_table_html = driver.execute_script("return arguments[0].outerHTML;", remark_table_element)
        soup_remark_table = BeautifulSoup(remark_table_html, 'html.parser')

        rows = soup_remark_table.find('tbody').find_all('tr')
        for row in rows:
            # ä½¿ç”¨BeautifulSoupçš„findæ–¹æ³•ï¼Œè€Œä¸æ˜¯Seleniumçš„find_element
            cells = row.find_all('td')
            if len(cells) == 2:
                # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯keyï¼Œç¬¬äºŒåˆ—æ˜¯value
                key = cells[0].text.strip().replace('[', '').replace(']', '').strip()
                value = cells[1].text.strip()
                if key:  # ç¡®ä¿keyä¸ä¸ºç©º
                    scraped_data["notes_map"][key] = value
    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æœªèƒ½ä½¿ç”¨ Selenium å®šä½åˆ°å¤‡æ³¨è¡¨æ ¼æˆ–æå–å¤‡æ³¨æ•°æ®ã€‚é”™è¯¯: {e}")

    print(f"âœ… æŠ“å–å®Œæˆï¼šæ‰¾åˆ° {len(scraped_data['columns_data'])} åˆ—æ•°æ®ï¼Œ{len(scraped_data['notes_map'])} æ¡å¤‡æ³¨ã€‚")
    return scraped_data


def organize_data_locally(table_name, scraped_data):
    """
    ä½¿ç”¨çº¯Pythonä»£ç å°†æŠ“å–çš„æ•°æ®æ•´ç†æˆæœ€ç»ˆçš„JSONç»“æ„ã€‚
    è¿™æ˜¯å¯¹ organize_data_with_ai å‡½æ•°çš„ç›´æ¥æ›¿ä»£ã€‚
    """
    print("ğŸ æ­£åœ¨ä½¿ç”¨æœ¬åœ°ä»£ç è¿›è¡Œæ•°æ®æ•´åˆä¸æ¸…æ´—...")

    # 1. é¢„å¤„ç†åˆ—æ•°æ®ï¼šå°†å¤‡æ³¨IDæ›¿æ¢ä¸ºå®é™…å¤‡æ³¨å†…å®¹
    processed_columns = []
    notes_map = scraped_data.get("notes_map", {})
    for col in scraped_data.get("columns_data", []):
        processed_col = col.copy()  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        remark_key = processed_col.get("å¤‡æ³¨")
        if remark_key and remark_key in notes_map:
            processed_col["å¤‡æ³¨"] = notes_map[remark_key]
        processed_columns.append(processed_col)

    # 2. æ„å»ºæœ€ç»ˆçš„JSONå¯¹è±¡
    final_table_definition = {
        "tableName": table_name,
        "tableChiName": scraped_data.get("basic_info", {}).get("tableChiName", ""),
        "description": scraped_data.get("basic_info", {}).get("description", ""),
        "tableUpdateTime": scraped_data.get("basic_info", {}).get("tableUpdateTime", ""),
        "key": scraped_data.get("basic_info", {}).get("key", ""),
        "columns": processed_columns
    }

    print("âœ… æœ¬åœ°æ•°æ®æ•´ç†å®Œæˆã€‚")
    return final_table_definition


def organize_data_with_ai(table_name, scraped_data):
    print("ğŸ¤– æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œæ•°æ®æ•´åˆä¸æ¸…æ´—...")
    try:
        http_client = httpx.Client(verify=False)
        client = OpenAI(api_key=CONFIG["api_key"], base_url=CONFIG["base_url"], http_client=http_client)

        # --- ç»ˆæä¿®æ­£ 2: å›ºåŒ–å¯¹AIçš„æŒ‡ä»¤ï¼Œå¼ºåˆ¶å…¶éµå¾ªJSONæ ¼å¼ ---
        system_prompt = (
            "You are a data structuring expert. Your job is to assemble raw, pre-scraped data parts into a final, clean JSON object representing a database table definition. "
            "You MUST strictly follow the output format specified in the user prompt."
        )

        # é¢„å¤„ç†columnsï¼Œæ›¿æ¢å¤‡æ³¨
        processed_columns = []
        for col in scraped_data["columns_data"]:
            processed_col = col.copy()
            remark_key = processed_col.get("å¤‡æ³¨")
            if remark_key and remark_key in scraped_data["notes_map"]:
                processed_col["å¤‡æ³¨"] = scraped_data["notes_map"][remark_key]
            processed_columns.append(processed_col)

        user_prompt = (
                f"Generate a JSON object for the table '{table_name}' with the following structure and data.\n"
                f"Use the provided basic_info, columns_data, and notes_map to populate the fields.\n"
                f"Ensure that the 'å¤‡æ³¨' field in each column object contains the full, replaced remark text from notes_map.\n\n"
                "```json\n"
                f"    \"{table_name}\": {{\n"
                "        \"tableName\": \"{table_name}\",\n"
                "        \"tableChiName\": \"" + scraped_data["basic_info"].get("tableChiName", "") + "\",\n"
                                                                                                      "        \"status\": \"" +
                scraped_data["basic_info"].get("status", "") + "\",\n"
                                                               f"        \"columns\": {json.dumps(processed_columns, indent=4, ensure_ascii=False).replace('\n', '\n').replace('"', '"')}\n"
                                                               "    }}\n"
                                                               "```\n"
                                                               "Return ONLY the final, assembled JSON object and nothing else. Do NOT add any extra text or explanations."
        )
        response = client.chat.completions.create(
            model=CONFIG["model_name"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            stream=False
        )
        response_text = response.choices[0].message.content
        json_part = response_text[response_text.find('{'):response_text.rfind('}') + 1]
        return json.loads(json_part)[table_name]
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œæ•°æ®æ•´ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(traceback.format_exc())
        return None


# --- æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ ---
def process_table_details_page(driver, table_name):
    """å¤„ç†è¡¨è¯¦æƒ…é¡µï¼Œæå–ã€æ•´ç†å¹¶ä¿å­˜æ•°æ®"""
    print("--- å¯¼èˆªåˆ°è¯¦æƒ…é¡µæˆåŠŸï¼Œå¼€å§‹æ•°æ®æå–æµç¨‹ --- ")
    scraped_data = {"columns_data": []}  # åˆå§‹åŒ–ä¸ºç©ºï¼Œç¡®ä¿å³ä½¿å¤±è´¥ä¹Ÿæœ‰æ­¤é”®
    table_row_locator = (By.CSS_SELECTOR, "table.table-column.table-interval-bg tbody tr")

    for attempt in range(3):  # æœ€å¤šå°è¯•3æ¬¡
        try:
            wait = WebDriverWait(driver, CONFIG["explicit_wait_timeout"])
            print(f"â³ å°è¯• {attempt + 1}/3: æ­£åœ¨ç­‰å¾…å…ƒç´ å‡ºç°: {table_row_locator}")
            wait.until(EC.presence_of_element_located(table_row_locator))
            random_sleep()  # å¢åŠ éšæœºç­‰å¾…ï¼Œç¡®ä¿é¡µé¢å®Œå…¨æ¸²æŸ“
            print("--- åˆ—è¡¨æ ¼å†…å®¹å·²åŠ è½½ï¼Œå¼€å§‹æŠ“å–æ•°æ® --- ")

            scraped_data = scrape_table_details(driver)
            if scraped_data and len(scraped_data.get("columns_data", [])) >= 2:
                print(f"âœ… å°è¯• {attempt + 1}/3: æˆåŠŸæŠ“å–åˆ° {len(scraped_data['columns_data'])} æ¡åˆ—ä¿¡æ¯ã€‚")
                break  # æˆåŠŸæŠ“å–åˆ°è¶³å¤Ÿæ•°æ®ï¼Œè·³å‡ºå¾ªç¯
            else:
                print(
                    f"âš ï¸ å°è¯• {attempt + 1}/3: æŠ“å–åˆ°çš„åˆ—ä¿¡æ¯ä¸è¶³2æ¡ ({len(scraped_data.get('columns_data', []))} æ¡)ã€‚")
                if attempt < 2:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™åˆ·æ–°é¡µé¢é‡è¯•
                    print("ğŸ”„ åˆ·æ–°é¡µé¢å¹¶é‡è¯•...")
                    driver.refresh()
                    random_sleep()  # åˆ·æ–°åç­‰å¾…é¡µé¢é‡æ–°åŠ è½½
                else:
                    print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå°†ä½¿ç”¨å½“å‰æŠ“å–åˆ°çš„æ•°æ®ã€‚")
        except Exception as e:
            print(f"âŒ å°è¯• {attempt + 1}/3: åœ¨ç­‰å¾…æˆ–æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(traceback.format_exc())
            if attempt < 2:
                print("ğŸ”„ åˆ·æ–°é¡µé¢å¹¶é‡è¯•...")
                driver.refresh()
                random_sleep()
            else:
                print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ— æ³•ç»§ç»­ã€‚")
                # è°ƒè¯•æ­¥éª¤ï¼šä¿å­˜å½“å‰é¡µé¢HTMLï¼Œä»¥ä¾¿åˆ†æ
                debug_file = "debug_page_source.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
                print(f"â„¹ï¸ ä¸ºäº†ä¾¿äºè°ƒè¯•ï¼Œå½“å‰çš„é¡µé¢HTMLå·²ä¿å­˜åˆ°: {os.path.abspath(debug_file)}")
                return  # å½»åº•å¤±è´¥ï¼Œè¿”å›

    if not scraped_data or not scraped_data.get("columns_data"):
        print("âŒ æœ€ç»ˆæœªèƒ½ä»é¡µé¢æŠ“å–åˆ°ä»»ä½•åˆ—ä¿¡æ¯ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return

    final_table_json = organize_data_locally(table_name, scraped_data)
    if final_table_json:
        print("ğŸ’¾ æ­£åœ¨ä»¥å¢é‡/æ›´æ–°æ¨¡å¼ä¿å­˜æ•°æ®...")
        database = load_json_file(CONFIG["output_json_file"])
        database[table_name] = final_table_json
        save_json_file(database, CONFIG["output_json_file"])
        print(f"âœ… æˆåŠŸå°†è¡¨ '{table_name}' çš„å®šä¹‰ä¿å­˜åˆ° {CONFIG['output_json_file']}ã€‚")
    else:
        print("âŒ AIæœªèƒ½æˆåŠŸæ•´ç†æ•°æ®ï¼Œæœ¬æ¬¡ä¸ä¿å­˜ã€‚")


def login_and_search(driver):
    driver.get(CONFIG["login_url"])
    cache = load_json_file(CONFIG["element_cache_file"])
    input("è¯·æ‰‹åŠ¨å®Œæˆç™»å½•åï¼Œå›åˆ°ç»ˆç«¯æŒ‰å›è½¦ç»§ç»­...")

    search_box_locator = cache.get("search_box")
    search_box = None
    if search_box_locator:
        print("ğŸ§  æ­£åœ¨å°è¯•ä»è®°å¿†ä¸­å®šä½æœç´¢æ¡†...")
        search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"], timeout=3)

    if not search_box:
        print("ğŸ¤” è®°å¿†å¤±æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œå¯åŠ¨å¤§æ¨¡å‹æ™ºèƒ½å‘ç°æ¨¡å¼...")
        html = driver.page_source
        search_box_locator = get_locator_from_ai(html, "the main search input box for table names")
        if search_box_locator:
            print(f"ğŸ¤– AIå‘ç°æœç´¢æ¡†å®šä½å™¨: {search_box_locator}")
            cache["search_box"] = search_box_locator
            save_json_file(cache, CONFIG["element_cache_file"])
            search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"])
        else:
            print("âŒ æ™ºèƒ½å‘ç°å¤±è´¥ï¼Œæ— æ³•æ‰¾åˆ°æœç´¢æ¡†ã€‚")
            return

    if not search_box:
        print("âŒ æœ€ç»ˆæœªèƒ½å®šä½åˆ°æœç´¢æ¡†ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    print("âœ… æˆåŠŸå®šä½åˆ°æœç´¢æ¡†ã€‚")

    for table_name in CONFIG["table_to_search"]:
        print(f"å¼€å§‹æŸ¥æ‰¾è¡¨ï¼š{table_name}")
        search_box.clear()
        search_box.send_keys(table_name)
        search_box.send_keys(Keys.ENTER)

        print(f"æ­£åœ¨æœç´¢ '{table_name}' çš„é“¾æ¥...")
        try:
            wait = WebDriverWait(driver, CONFIG["explicit_wait_timeout"])
            # å®šä½åˆ°æˆ‘ä»¬æƒ³è¦ç‚¹å‡»çš„é‚£ä¸ªç²¾ç¡®çš„é“¾æ¥
            link_xpath = f"//a[contains(@href, '/tableShow/') and normalize-space(.)='{table_name}']"
            link_element = wait.until(EC.presence_of_element_located((By.XPATH, link_xpath)))

            # --- ç›´æ¥æå–hrefå¹¶å¯¼èˆª ---
            target_url = link_element.get_attribute('href')
            print(f"âœ… æˆåŠŸå®šä½é“¾æ¥ï¼Œæå–åˆ°ç›®æ ‡URL: {target_url}")

            if not target_url:
                print("âŒ æ— æ³•ä»é“¾æ¥å…ƒç´ ä¸­æå–åˆ°hrefå±æ€§ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
                continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨

            print("ğŸš€ æ­£åœ¨ç›´æ¥å¯¼èˆªåˆ°ç›®æ ‡URL...")
            driver.get(target_url)

            # ç›´æ¥å¯¼èˆªåï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ç­‰å¾…æ–°è§†å›¾çš„æ ‡å¿—æ€§å…ƒç´ å‡ºç°
            print("â³ æ­£åœ¨ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½...")
            details_page_identifier = (By.CSS_SELECTOR, "table.table-column.table-interval-bg")
            wait.until(EC.presence_of_element_located(details_page_identifier))
            print("âœ… è¯¦æƒ…é¡µåŠ è½½æˆåŠŸã€‚")

            # ç°åœ¨å¯ä»¥å®‰å…¨åœ°è°ƒç”¨è¯¦æƒ…é¡µå¤„ç†å‡½æ•°
            process_table_details_page(driver, table_name)
            print(f"--- è¡¨ '{table_name}' æ•°æ®æå–æµç¨‹ç»“æŸ ---")

        except Exception as e:
            print(f"âš ï¸ åœ¨æŸ¥æ‰¾ã€å¯¼èˆªæˆ–å¤„ç† '{table_name}' æ—¶å¤±è´¥: {e}")
            print(traceback.format_exc())
            # ä¿å­˜å¤±è´¥æ—¶çš„é¡µé¢å¿«ç…§ï¼Œä»¥ä¾¿è°ƒè¯•
            debug_file = f"debug_page_source_final_{table_name}.html"
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            print(f"â„¹ï¸ ä¸ºäº†ä¾¿äºè°ƒè¯•ï¼Œæœ€ç»ˆå¤±è´¥æ—¶çš„é¡µé¢HTMLå·²ä¿å­˜åˆ°: {os.path.abspath(debug_file)}")
            print(traceback.format_exc())
        finally:
            # æ¯æ¬¡å¤„ç†å®Œä¸€ä¸ªè¡¨åï¼Œè¿”å›åˆ°æœç´¢é¡µé¢ï¼Œä»¥ä¾¿æœç´¢ä¸‹ä¸€ä¸ªè¡¨
            driver.get(CONFIG["login_url"])
            # é‡æ–°å®šä½æœç´¢æ¡†ï¼Œå› ä¸ºé¡µé¢å¯èƒ½åˆ·æ–°äº†
            search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"])
            if not search_box:
                print("âŒ é‡æ–°å®šä½æœç´¢æ¡†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†åç»­è¡¨ã€‚")
                break  # é€€å‡ºå¾ªç¯
            random_sleep()  # åœ¨å¤„ç†ä¸‹ä¸€ä¸ªè¡¨ä¹‹å‰ä¼‘çœ éšæœºæ—¶é—´


def main():
    driver = None
    try:
        driver = launch_browser()
        login_and_search(driver)
        print(f"æ“ä½œå®Œæˆï¼Œé¡µé¢å°†ä¿æŒæ‰“å¼€{CONFIG['final_observe_time']} ç§’... ")
        time.sleep(CONFIG['final_observe_time'])
    except Exception as e:
        print(f"ç¨‹åºå‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        print(traceback.format_exc())
    finally:
        if driver:
            print("å…³é—­æµè§ˆå™¨ã€‚")
            driver.quit()


if __name__ == "__main__":
    main()
