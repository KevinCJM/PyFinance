import json
import traceback
import os
import httpx
import re
import subprocess
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from openai import OpenAI
from bs4 import BeautifulSoup
import time
import random

# --- é…ç½®ä¸­å¿ƒ ---
CONFIG = {
    "login_url": "https://dd.gildata.com/#/login",
    "table_to_search": [
        "SecuMain",
        "HK_SecuMain", "MF_FundArchives", "MF_FundType", "MF_Transformation",
        "MF_KeyStockPortfolio", "MF_QDIIPortfolioDetail", "MF_BondPortifolioDetail", "MF_QDIIPortfolioDetail",
        "MF_FundPortifolioDetail", "MF_QDIIPortfolioDetail", "MF_BalanceSheetNew", "MF_BondPortifolioStru",
        "MF_AssetAllocationNew", "MF_StockPortfolioDetail", "LC_DIndicesForValuation"
    ],
    "api_key": "sk-a221b0c62c8a460693fe00a627d4598e",
    "base_url": "https://api.deepseek.com",
    "model_name": "deepseek-chat",
    "window_size": (1920, 1080),
    "explicit_wait_timeout": 20,
    "final_observe_time": 5,
    "element_cache_file": "element_cache.json",
    "output_json_file": "table_definitions.json",
    "min_sleep_time": 0.5,
    "max_sleep_time": 2.5,
}


# --- è®°å¿†/ç¼“å­˜/æ•°æ®åº“ åŠŸèƒ½ ---
def load_json_file(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}
    return {}


def save_json_file(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)


def random_sleep():
    sleep_time = random.uniform(CONFIG["min_sleep_time"], CONFIG["max_sleep_time"])
    print(f"ğŸ˜´ ä¼‘çœ  {sleep_time:.2f} ç§’...")
    time.sleep(sleep_time)


# --- æµè§ˆå™¨ä¸å…ƒç´ æ“ä½œ ---
def launch_browser():
    options = webdriver.ChromeOptions()
    if CONFIG.get("window_size"):
        width, height = CONFIG["window_size"]
        options.add_argument(f"--window-size={width},{height}")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


def find_element_safely(driver, by, value, timeout=CONFIG["explicit_wait_timeout"]):
    try:
        wait = WebDriverWait(driver, timeout)
        by_obj = getattr(By, by.upper())
        return wait.until(EC.presence_of_element_located((by_obj, value)))
    except Exception:
        print(traceback.format_exc())
        return None


# --- æ•°æ®æå–ä¸AIæ•´ç†æ¨¡å— ---
def simplify_html(html_source):
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'link', 'meta']):
        tag.decompose()
    body = soup.find('body')
    if body:
        return str(body.prettify())
    return ""


def get_locator_from_ai(html_source, element_description):
    print("ğŸ¤– æ­£åœ¨ç²¾ç®€HTMLä»¥é€‚åº”æ¨¡å‹ä¸Šä¸‹æ–‡...")
    simplified_html = simplify_html(html_source)
    if len(simplified_html) > 200000:
        print(f"âŒ ç²¾ç®€åçš„HTMLä»ç„¶è¿‡é•¿ ({len(simplified_html)} chars)ï¼Œè·³è¿‡APIè°ƒç”¨ã€‚")
        return None

    print("ğŸ¤– æ­£å¼è°ƒç”¨å¤§æ¨¡å‹APIè¿›è¡Œåˆ†æ...")
    try:
        http_client = httpx.Client(verify=False)
        client = OpenAI(api_key=CONFIG["api_key"], base_url=CONFIG["base_url"], http_client=http_client)
        system_prompt = (
            "You are an expert web automation assistant. Your task is to analyze HTML source code "
            "and return a single, precise, and robust Selenium locator for a requested element. "
            "You must return the result as a JSON object with two keys: 'by' and 'value'. "
            "The 'by' key must be one of the following strings: 'ID', 'NAME', 'CLASS_NAME', 'TAG_NAME', 'LINK_TEXT', 'PARTIAL_LINK_TEXT', 'CSS_SELECTOR', 'XPATH'. "
            "The 'value' key is the corresponding locator string."
        )
        user_prompt = (
            f"Based on the following HTML, find the locator for the '{element_description}'.\n\n"
            f"HTML:\n```html\n{simplified_html}\n```\n\n"
            "Return only the JSON object."
        )
        response = client.chat.completions.create(
            model=CONFIG["model_name"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            stream=False
        )
        response_text = response.choices[0].message.content
        print(f"ğŸ¤– å¤§æ¨¡å‹è¿”å›åŸå§‹ç»“æœ: {response_text}")
        json_part = response_text[response_text.find('{'):response_text.rfind('}') + 1]
        locator = json.loads(json_part)
        if isinstance(locator, dict) and 'by' in locator and 'value' in locator:
            return locator
        else:
            print("âŒ å¤§æ¨¡å‹è¿”å›çš„JSONæ ¼å¼ä¸æ­£ç¡®ã€‚")
            return None
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(traceback.format_exc())
        return None


def scrape_table_details(driver):
    """ä»è¡¨è¯¦æƒ…é¡µHTMLä¸­æŠ“å–æ‰€æœ‰ä¿¡æ¯(v14.4 ç»ˆæä¿®æ­£æå–ä¸æ ¼å¼)"""
    print("ğŸ” æ­£åœ¨ä½¿ç”¨Seleniumç›´æ¥æå–é¡µé¢å…ƒç´ ...")
    scraped_data = {"basic_info": {}, "columns_data": [], "notes_map": {}}

    # --- ç»ˆæä¿®æ­£ 1: ä½¿ç”¨æ›´ç²¾ç¡®çš„XPathæå–åŸºæœ¬ä¿¡æ¯ ---
    # å°è¯•æå–â€œè¡¨ä¸­æ–‡åâ€åŠå…¶ä»–åŸºæœ¬ä¿¡æ¯
    try:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        # æŸ¥æ‰¾åŒ…å«ä¸­æ–‡è¡¨åçš„spanæ ‡ç­¾
        chinese_name_span = soup.find('span', {'ng-bind-html': re.compile(r'table\.tableChiName')})
        if chinese_name_span:
            scraped_data["basic_info"]["tableChiName"] = chinese_name_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'è¡¨ä¸­æ–‡å'ã€‚")

        # æå– description
        description_span = soup.find('span', {'ng-bind-html': re.compile(r'table\.description')})
        if description_span:
            scraped_data["basic_info"]["description"] = description_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'description'ã€‚")

        # æå– tableUpdateTime
        table_update_time_span = soup.find('span', {'ng-bind': 'table.tableUpdateTime'})
        if table_update_time_span:
            scraped_data["basic_info"]["tableUpdateTime"] = table_update_time_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'tableUpdateTime'ã€‚")

        # æå– key (ä¸šåŠ¡å”¯ä¸€æ€§)
        key_span = soup.find('span', {'ng-bind': "index.columnName || 'æ— '"})
        if key_span:
            scraped_data["basic_info"]["key"] = key_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'key'ã€‚")

    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æå–åŸºæœ¬ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    # 2. æå–åˆ—ä¿¡æ¯ (é€»è¾‘ä¸å˜)
    try:
        column_table_element = driver.find_element(By.CSS_SELECTOR, 'table.table-column.table-interval-bg')
        column_table_html = driver.execute_script("return arguments[0].outerHTML;", column_table_element)
        soup_column_table = BeautifulSoup(column_table_html, 'html.parser')

        headers = [th.text.strip() for th in soup_column_table.find('thead').find_all('th')]
        # ä¿®æ­£ï¼šng-repeatå¯èƒ½åœ¨tbodyä¸Šï¼Œè€Œä¸æ˜¯trä¸Šï¼ŒæŸ¥æ‰¾æ‰€æœ‰træ›´ç¨³å¦¥
        rows = soup_column_table.find('tbody').find_all('tr')

        for row in rows:
            cells = row.find_all('td')
            if not cells:
                continue  # è·³è¿‡ç©ºè¡Œæˆ–è¡¨å¤´å†…çš„è¡Œ
            col_dict = {}
            for i, header in enumerate(headers):
                if i < len(cells):
                    col_dict[header] = cells[i].text.strip()
            if col_dict:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—å…¸
                scraped_data["columns_data"].append(col_dict)
    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æœªèƒ½ä½¿ç”¨ Selenium å®šä½åˆ°åˆ—è¡¨æ ¼æˆ–æå–åˆ—æ•°æ®ã€‚é”™è¯¯: {e}")

    # --- æ ¸å¿ƒä¿®æ­£ 2: ä¿®æ­£å¤‡æ³¨ä¿¡æ¯æå– ---
    try:
        remark_table_element = driver.find_element(By.CSS_SELECTOR, 'table.table-remark')
        rows = remark_table_element.find_elements(By.TAG_NAME, 'tr')

        for row in rows:
            cells = row.find_elements(By.TAG_NAME, 'td')
            if len(cells) == 2:
                key_element = cells[0]
                value_element = cells[1]  # This is the WebElement for the remark content

                key = key_element.text.strip().replace('[', '').replace(']', '').strip()
                initial_value = value_element.text.strip()

                # Check for "æ›´å¤š" button and click if present
                more_button = None
                try:
                    # Look for a span or a tag with text 'æ›´å¤š' within the value_element
                    more_button = value_element.find_element(By.XPATH, ".//span[text()='æ›´å¤š']")
                except:
                    try:
                        more_button = value_element.find_element(By.XPATH, ".//a[text()='æ›´å¤š']")
                    except:
                        pass  # No 'æ›´å¤š' button found

                if more_button:
                    print(f"â„¹ï¸ å‘ç°å¤‡æ³¨ '{key}' å­˜åœ¨ 'æ›´å¤š' æŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»å±•å¼€...")
                    driver.execute_script("arguments[0].click();", more_button)  # Use JS click for robustness
                    time.sleep(1)  # Give time for content to expand
                    # Re-get the text after expansion
                    value = value_element.text.strip()
                    print(f"âœ… å¤‡æ³¨ '{key}' å·²å±•å¼€ã€‚")
                else:
                    value = initial_value  # No 'æ›´å¤š' button, use initial value

                if key:  # ç¡®ä¿keyä¸ä¸ºç©º
                    scraped_data["notes_map"][key] = value
    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æœªèƒ½ä½¿ç”¨ Selenium å®šä½åˆ°å¤‡æ³¨è¡¨æ ¼æˆ–æå–å¤‡æ³¨æ•°æ®ã€‚é”™è¯¯: {e}")

    print(f"âœ… æŠ“å–å®Œæˆï¼šæ‰¾åˆ° {len(scraped_data['columns_data'])} åˆ—æ•°æ®ï¼Œ{len(scraped_data['notes_map'])} æ¡å¤‡æ³¨ã€‚")
    return scraped_data


def simplify_comment_with_llm(comment_text):
    print(comment_text)
    """
    ä½¿ç”¨å¤§æ¨¡å‹ç®€åŒ–å•ä¸ªå¤‡æ³¨ä¿¡æ¯ï¼Œæ ¹æ®å†…å®¹é€‰æ‹©ä¸åŒçš„æç¤ºè¯ï¼Œå¹¶ä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å€¼æ˜ å°„ã€‚
    """
    if not comment_text or comment_text.strip() == "":
        return ""

    print(f"ğŸ¤– æ­£åœ¨å¤„ç†å¤‡æ³¨: '{comment_text[:50]}...' ")

    # ä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–â€œæ•°å­—-æè¿°â€åˆ—è¡¨
    if "CT_SystemConst" in comment_text and "DMå­—æ®µ" in comment_text:
        value_pairs = re.findall(r'(\d+)[-â€”]([^\s,ï¼Œã€‚ï¼›ï¼›\n\r]+)', comment_text)
        if value_pairs:
            formatted = [f"{code}-{desc}" for code, desc in value_pairs]
            extracted_values = ", ".join(formatted)
            print(f"âœ… æ­£åˆ™è¡¨è¾¾å¼æå–åˆ°å€¼æ˜ å°„: '{extracted_values[:50]}...' ")
            return extracted_values

    # Fallback to LLM if regex doesn't apply or doesn't find anything
    try:
        http_client = httpx.Client(verify=False)
        client = OpenAI(api_key=CONFIG["api_key"], base_url=CONFIG["base_url"], http_client=http_client)

        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åº“æ–‡æ¡£åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç®€åŒ–æ•°æ®åº“å­—æ®µçš„å¤‡æ³¨ä¿¡æ¯ã€‚"
            "ä½ ä¼šæ”¶åˆ°ä¸€ä¸ªå¤‡æ³¨æ–‡æœ¬ã€‚"
            "è¯·ç”¨æœ€ç®€æ´çš„è¯­è¨€æ€»ç»“å…¶æ ¸å¿ƒå«ä¹‰ã€ä¸å…¶ä»–è¡¨çš„å…³è”æˆ–å…³é”®ä¸šåŠ¡é€»è¾‘ï¼Œå»é™¤å†—ä½™çš„è§£é‡Šæ€§æ–‡å­—ã€‚"
            "å¦‚æœå¤‡æ³¨å·²ç»éå¸¸ç®€æ´ï¼Œè¯·ç›´æ¥è¿”å›åŸå§‹å¤‡æ³¨ã€‚"
            "åªè¿”å›ç®€åŒ–åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜ã€‚"
        )
        user_prompt = f"è¯·ç®€åŒ–ä»¥ä¸‹å¤‡æ³¨ï¼š\n\n{comment_text}"

        response = client.chat.completions.create(
            model=CONFIG["model_name"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„ç»“æœ
            stream=False
        )
        simplified_text = response.choices[0].message.content.strip()
        print(f"âœ… å¤§æ¨¡å‹ç®€åŒ–å®Œæˆ: '{simplified_text[:50]}...' ")
        return simplified_text
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹ç®€åŒ–å¤‡æ³¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(traceback.format_exc())
        return comment_text  # å¤±è´¥æ—¶è¿”å›åŸå§‹å¤‡æ³¨


def organize_data_locally(table_name, scraped_data):
    """
    ä½¿ç”¨çº¯Pythonä»£ç å°†æŠ“å–çš„æ•°æ®æ•´ç†æˆæœ€ç»ˆçš„JSONç»“æ„ï¼Œå¹¶å¯¹æ¯ä¸ªå¤‡æ³¨è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œç®€åŒ–ã€‚
    """
    print("ğŸ æ­£åœ¨ä½¿ç”¨æœ¬åœ°ä»£ç è¿›è¡Œæ•°æ®æ•´åˆä¸æ¸…æ´—...")

    processed_columns = []
    notes_map = scraped_data.get("notes_map", {})

    for col in scraped_data.get("columns_data", []):
        processed_col = col.copy()  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        remark_key = processed_col.get("å¤‡æ³¨")
        if remark_key and remark_key in notes_map:
            original_remark = notes_map[remark_key]
            simplified_remark = simplify_comment_with_llm(original_remark)
            processed_col["å¤‡æ³¨"] = simplified_remark
        processed_columns.append(processed_col)

    # æ„å»ºæœ€ç»ˆçš„JSONå¯¹è±¡
    final_table_definition = {
        "tableName": table_name,
        "tableChiName": scraped_data.get("basic_info", {}).get("tableChiName", ""),
        "description": scraped_data.get("basic_info", {}).get("description", ""),
        "tableUpdateTime": scraped_data.get("basic_info", {}).get("tableUpdateTime", ""),
        "key": scraped_data.get("basic_info", {}).get("key", ""),
        "columns": processed_columns
    }

    print("âœ… æœ¬åœ°æ•°æ®æ•´ç†å®Œæˆã€‚")
    return final_table_definition


# --- æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ ---
def process_table_details_page(driver, table_name):
    """å¤„ç†è¡¨è¯¦æƒ…é¡µï¼Œæå–ã€æ•´ç†å¹¶ä¿å­˜æ•°æ®"""
    print("--- å¯¼èˆªåˆ°è¯¦æƒ…é¡µæˆåŠŸï¼Œå¼€å§‹æ•°æ®æå–æµç¨‹ --- ")
    scraped_data = {"columns_data": []}  # åˆå§‹åŒ–ä¸ºç©ºï¼Œç¡®ä¿å³ä½¿å¤±è´¥ä¹Ÿæœ‰æ­¤é”®
    table_row_locator = (By.CSS_SELECTOR, "table.table-column.table-interval-bg tbody tr")

    for attempt in range(3):  # æœ€å¤šå°è¯•3æ¬¡
        try:
            wait = WebDriverWait(driver, CONFIG["explicit_wait_timeout"])
            print(f"â³ å°è¯• {attempt + 1}/3: æ­£åœ¨ç­‰å¾…å…ƒç´ å‡ºç°: {table_row_locator}")
            wait.until(EC.presence_of_element_located(table_row_locator))
            random_sleep()  # å¢åŠ éšæœºç­‰å¾…ï¼Œç¡®ä¿é¡µé¢å®Œå…¨æ¸²æŸ“
            print("--- åˆ—è¡¨æ ¼å†…å®¹å·²åŠ è½½ï¼Œå¼€å§‹æŠ“å–æ•°æ® --- ")

            scraped_data = scrape_table_details(driver)
            if scraped_data and len(scraped_data.get("columns_data", [])) >= 2:
                print(f"âœ… å°è¯• {attempt + 1}/3: æˆåŠŸæŠ“å–åˆ° {len(scraped_data['columns_data'])} æ¡åˆ—ä¿¡æ¯ã€‚")
                break  # æˆåŠŸæŠ“å–åˆ°è¶³å¤Ÿæ•°æ®ï¼Œè·³å‡ºå¾ªç¯
            else:
                print(
                    f"âš ï¸ å°è¯• {attempt + 1}/3: æŠ“å–åˆ°çš„åˆ—ä¿¡æ¯ä¸è¶³2æ¡ ({len(scraped_data.get('columns_data', []))} æ¡)ã€‚")
                if attempt < 2:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™åˆ·æ–°é¡µé¢é‡è¯•
                    print("ğŸ”„ åˆ·æ–°é¡µé¢å¹¶é‡è¯•...")
                    driver.refresh()
                    random_sleep()  # åˆ·æ–°åç­‰å¾…é¡µé¢é‡æ–°åŠ è½½
                else:
                    print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå°†ä½¿ç”¨å½“å‰æŠ“å–åˆ°çš„æ•°æ®ã€‚")
        except Exception as e:
            print(f"âŒ å°è¯• {attempt + 1}/3: åœ¨ç­‰å¾…æˆ–æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(traceback.format_exc())
            if attempt < 2:
                print("ğŸ”„ åˆ·æ–°é¡µé¢å¹¶é‡è¯•...")
                driver.refresh()
                random_sleep()
            else:
                print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ— æ³•ç»§ç»­ã€‚")
                # è°ƒè¯•æ­¥éª¤ï¼šä¿å­˜å½“å‰é¡µé¢HTMLï¼Œä»¥ä¾¿åˆ†æ
                debug_file = "debug_page_source.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
                print(f"â„¹ï¸ ä¸ºäº†ä¾¿äºè°ƒè¯•ï¼Œå½“å‰çš„é¡µé¢HTMLå·²ä¿å­˜åˆ°: {os.path.abspath(debug_file)}")
                return  # å½»åº•å¤±è´¥ï¼Œè¿”å›

    if not scraped_data or not scraped_data.get("columns_data"):
        print("âŒ æœ€ç»ˆæœªèƒ½ä»é¡µé¢æŠ“å–åˆ°ä»»ä½•åˆ—ä¿¡æ¯ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return

    final_table_json = organize_data_locally(table_name, scraped_data)
    if final_table_json:
        print("ğŸ’¾ æ­£åœ¨ä»¥å¢é‡/æ›´æ–°æ¨¡å¼ä¿å­˜æ•°æ®...")
        database = load_json_file(CONFIG["output_json_file"])
        database[table_name] = final_table_json
        save_json_file(database, CONFIG["output_json_file"])
        print(f"âœ… æˆåŠŸå°†è¡¨ '{table_name}' çš„å®šä¹‰ä¿å­˜åˆ° {CONFIG['output_json_file']}ã€‚")
    else:
        print("âŒ AIæœªèƒ½æˆåŠŸæ•´ç†æ•°æ®ï¼Œæœ¬æ¬¡ä¸ä¿å­˜ã€‚")


def login_and_search(driver):
    driver.get(CONFIG["login_url"])
    cache = load_json_file(CONFIG["element_cache_file"])
    input("è¯·æ‰‹åŠ¨å®Œæˆç™»å½•åï¼Œå›åˆ°ç»ˆç«¯æŒ‰å›è½¦ç»§ç»­...")

    search_box_locator = cache.get("search_box")
    search_box = None
    if search_box_locator:
        print("ğŸ§  æ­£åœ¨å°è¯•ä»è®°å¿†ä¸­å®šä½æœç´¢æ¡†...")
        search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"], timeout=3)

    if not search_box:
        print("ğŸ¤” è®°å¿†å¤±æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œå¯åŠ¨å¤§æ¨¡å‹æ™ºèƒ½å‘ç°æ¨¡å¼...")
        html = driver.page_source
        search_box_locator = get_locator_from_ai(html, "the main search input box for table names")
        if search_box_locator:
            print(f"ğŸ¤– AIå‘ç°æœç´¢æ¡†å®šä½å™¨: {search_box_locator}")
            cache["search_box"] = search_box_locator
            save_json_file(cache, CONFIG["element_cache_file"])
            search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"])
        else:
            print("âŒ æ™ºèƒ½å‘ç°å¤±è´¥ï¼Œæ— æ³•æ‰¾åˆ°æœç´¢æ¡†ã€‚")
            return

    if not search_box:
        print("âŒ æœ€ç»ˆæœªèƒ½å®šä½åˆ°æœç´¢æ¡†ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    print("âœ… æˆåŠŸå®šä½åˆ°æœç´¢æ¡†ã€‚")

    for table_name in CONFIG["table_to_search"]:
        print(f"å¼€å§‹æŸ¥æ‰¾è¡¨ï¼š{table_name}")
        search_box.clear()
        search_box.send_keys(table_name)
        search_box.send_keys(Keys.ENTER)

        print(f"æ­£åœ¨æœç´¢ '{table_name}' çš„é“¾æ¥...")
        try:
            wait = WebDriverWait(driver, CONFIG["explicit_wait_timeout"])
            # å®šä½åˆ°æˆ‘ä»¬æƒ³è¦ç‚¹å‡»çš„é‚£ä¸ªç²¾ç¡®çš„é“¾æ¥
            link_xpath = f"//a[contains(@href, '/tableShow/') and normalize-space(.)='{table_name}']"
            link_element = wait.until(EC.presence_of_element_located((By.XPATH, link_xpath)))

            # --- ç›´æ¥æå–hrefå¹¶å¯¼èˆª ---
            target_url = link_element.get_attribute('href')
            print(f"âœ… æˆåŠŸå®šä½é“¾æ¥ï¼Œæå–åˆ°ç›®æ ‡URL: {target_url}")

            if not target_url:
                print("âŒ æ— æ³•ä»é“¾æ¥å…ƒç´ ä¸­æå–åˆ°hrefå±æ€§ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
                continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨

            print("ğŸš€ æ­£åœ¨ç›´æ¥å¯¼èˆªåˆ°ç›®æ ‡URL...")
            driver.get(target_url)

            # ç›´æ¥å¯¼èˆªåï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ç­‰å¾…æ–°è§†å›¾çš„æ ‡å¿—æ€§å…ƒç´ å‡ºç°
            print("â³ æ­£åœ¨ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½...")
            details_page_identifier = (By.CSS_SELECTOR, "table.table-column.table-interval-bg")
            wait.until(EC.presence_of_element_located(details_page_identifier))
            print("âœ… è¯¦æƒ…é¡µåŠ è½½æˆåŠŸã€‚")

            # ç°åœ¨å¯ä»¥å®‰å…¨åœ°è°ƒç”¨è¯¦æƒ…é¡µå¤„ç†å‡½æ•°
            process_table_details_page(driver, table_name)
            print(f"--- è¡¨ '{table_name}' æ•°æ®æå–æµç¨‹ç»“æŸ ---")

        except Exception as e:
            print(f"âš ï¸ åœ¨æŸ¥æ‰¾ã€å¯¼èˆªæˆ–å¤„ç† '{table_name}' æ—¶å¤±è´¥: {e}")
            print(traceback.format_exc())
            # ä¿å­˜å¤±è´¥æ—¶çš„é¡µé¢å¿«ç…§ï¼Œä»¥ä¾¿è°ƒè¯•
            debug_file = f"debug_page_source_final_{table_name}.html"
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            print(f"â„¹ï¸ ä¸ºäº†ä¾¿äºè°ƒè¯•ï¼Œæœ€ç»ˆå¤±è´¥æ—¶çš„é¡µé¢HTMLå·²ä¿å­˜åˆ°: {os.path.abspath(debug_file)}")
            print(traceback.format_exc())
        finally:
            # æ¯æ¬¡å¤„ç†å®Œä¸€ä¸ªè¡¨åï¼Œè¿”å›åˆ°æœç´¢é¡µé¢ï¼Œä»¥ä¾¿æœç´¢ä¸‹ä¸€ä¸ªè¡¨
            driver.get(CONFIG["login_url"])
            # é‡æ–°å®šä½æœç´¢æ¡†ï¼Œå› ä¸ºé¡µé¢å¯èƒ½åˆ·æ–°äº†
            search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"])
            if not search_box:
                print("âŒ é‡æ–°å®šä½æœç´¢æ¡†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†åç»­è¡¨ã€‚")
                break  # é€€å‡ºå¾ªç¯
            random_sleep()  # åœ¨å¤„ç†ä¸‹ä¸€ä¸ªè¡¨ä¹‹å‰ä¼‘çœ éšæœºæ—¶é—´


def main():
    caffeinate_process = None
    if sys.platform == "darwin":
        print("ï£¿ æ£€æµ‹åˆ°macOSï¼Œå¯åŠ¨caffeinateå‘½ä»¤é˜²æ­¢ç³»ç»Ÿä¼‘çœ ...")
        # -d: é˜²æ­¢æ˜¾ç¤ºå™¨ä¼‘çœ 
        # -i: é˜²æ­¢ç³»ç»Ÿç©ºé—²æ—¶ä¼‘çœ 
        # -m: é˜²æ­¢ç£ç›˜ç©ºé—²æ—¶ä¼‘çœ 
        # -s: åœ¨è¿æ¥ç”µæºæ—¶é˜²æ­¢ç³»ç»Ÿä¼‘çœ 
        caffeinate_process = subprocess.Popen(["caffeinate", "-d", "-i", "-m", "-s"])

    driver = None
    try:
        driver = launch_browser()
        login_and_search(driver)
        print(f"æ“ä½œå®Œæˆï¼Œé¡µé¢å°†ä¿æŒæ‰“å¼€{CONFIG['final_observe_time']} ç§’... ")
        time.sleep(CONFIG['final_observe_time'])
    except Exception as e:
        print(f"ç¨‹åºå‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        print(traceback.format_exc())
    finally:
        if driver:
            print("å…³é—­æµè§ˆå™¨ã€‚")
            driver.quit()

        if caffeinate_process:
            print("ï£¿ æ­£åœ¨ç»ˆæ­¢caffeinateè¿›ç¨‹...")
            caffeinate_process.terminate()
            caffeinate_process.wait()
            print("âœ… caffeinateè¿›ç¨‹å·²ç»ˆæ­¢ï¼Œç³»ç»Ÿå¯æ­£å¸¸ä¼‘çœ ã€‚")


if __name__ == "__main__":
    main()
    # str = '''è¯åˆ¸å¸‚åœº(SecuMarket)ä¸(CT_SystemConst)è¡¨ä¸­çš„DMå­—æ®µå…³è”ï¼Œä»¤LB = 201 AND DM IN (10,12,13,14,15,16,18,40,49,50,52,54,55,56,65,66,67,68,69,70,71,72,73,75,76,77,78,79,80,81,83,84,85,86,87,88,89,90,93,94,95,96,99,100,101,102,103,104,105,106,107,110,161,162,180,200,202,210,230,240,260,280,310,320,390,400,620,630,631,640,641,650,653,654,655,657,658,659,660,661,662,663,664,666,667,66302,66303,66305)ï¼Œå¾—åˆ°è¯åˆ¸å¸‚åœºçš„å…·ä½“æè¿°ï¼š10-ä¸Šæµ·æœŸè´§äº¤æ˜“æ‰€ï¼Œ12-ä¸­å›½é“¶è¡Œé—´å¤–æ±‡å¸‚åœºï¼Œ13-å¤§è¿å•†å“äº¤æ˜“æ‰€ï¼Œ14-ä¸Šæµ·é»„é‡‘äº¤æ˜“æ‰€ï¼Œ15-éƒ‘å·å•†å“äº¤æ˜“æ‰€ï¼Œ16-ä¸Šæµ·ç¥¨æ®äº¤æ˜“æ‰€ï¼Œ18-åŒ—äº¬è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ40-èŠåŠ å“¥å•†ä¸šäº¤æ˜“æ‰€ï¼Œ49-æ¾³å¤§åˆ©äºšè¯åˆ¸äº¤æ˜“æ‰€ï¼Œ50-æ–°è¥¿å…°è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ52-åŸƒåŠå¼€ç½—åŠäºšå†å±±å¤§è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ54-é˜¿æ ¹å»·å¸ƒå®œè¯ºæ–¯è‰¾åˆ©æ–¯è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ55-å·´è¥¿åœ£ä¿ç½—è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ56-å¢¨è¥¿å“¥è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ65-å°åº¦å°¼è¥¿äºšè¯åˆ¸äº¤æ˜“æ‰€ï¼Œ66-æ³°å›½è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ67-éŸ©å›½é¦–å°”è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ68-ä¸œäº¬è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ69-æ–°åŠ å¡è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ70-å°æ¹¾è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ71-æŸœå°äº¤æ˜“å¸‚åœºï¼Œ72-é¦™æ¸¯è”äº¤æ‰€ï¼Œ73-ä¸€çº§å¸‚åœºï¼Œ75-äºšæ´²å…¶ä»–äº¤æ˜“æ‰€ï¼Œ76-ç¾å›½è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ77-ç¾å›½çº³æ–¯è¾¾å…‹è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ78-çº½çº¦è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ79-ç¾å›½å…¶ä»–äº¤æ˜“å¸‚åœºï¼Œ80-åŠ æ‹¿å¤§å¤šä¼¦å¤šè¯åˆ¸äº¤æ˜“æ‰€ï¼Œ81-ä¸‰æ¿å¸‚åœºï¼Œ83-ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ84-å…¶ä»–å¸‚åœºï¼Œ85-ä¼¦æ•¦è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ86-æ³•å›½å·´é»è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ87-å¾·å›½æ³•å…°å…‹ç¦è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ88-æ¬§æ´²å…¶ä»–äº¤æ˜“æ‰€ï¼Œ89-é“¶è¡Œé—´å€ºåˆ¸å¸‚åœºï¼Œ90-æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ93-ä¸Šæµ·é“¶è¡Œé—´åŒä¸šæ‹†å€Ÿå¸‚åœºï¼Œ94-ç‘å£«è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ95-è·å…°é˜¿å§†æ–¯ç‰¹ä¸¹è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ96-çº¦ç¿°å†…æ–¯å ¡è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ99-ä¸œäº¬åŒä¸šæ‹†å€Ÿå¸‚åœºï¼Œ100-ç¾å›½å›½å€ºå›è´­å¸‚åœºï¼Œ101-ä¼¦æ•¦é“¶è¡ŒåŒä¸šæ‹†å€Ÿå¸‚åœºï¼Œ102-é¦™æ¸¯é“¶è¡ŒåŒä¸šæ‹†å€Ÿå¸‚åœºï¼Œ103-æ–°åŠ å¡é“¶è¡ŒåŒä¸šæ‹†å€Ÿå¸‚åœºï¼Œ104-ä¸­å›½é“¶è¡ŒåŒä¸šæ‹†å€Ÿå¸‚åœºï¼Œ105-æ¬§å…ƒé“¶è¡ŒåŒä¸šæ‹†å€Ÿå¸‚åœºï¼Œ106-å¸ƒé²å¡å°”è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ107-é›…åŠ è¾¾è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ110-ä»¥è‰²åˆ—ç‰¹æ‹‰ç»´å¤«è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ161-æ„å¤§åˆ©è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ162-å“¥æœ¬å“ˆæ ¹è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ180-æŒªå¨å¥¥æ–¯é™†è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ200-æ–¯å¾·å“¥å°”æ‘©è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ202-ä¼Šæ–¯å¦å¸ƒå°”è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ210-å°åº¦å›½å®¶è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ230-å¥¥åœ°åˆ©ç»´ä¹Ÿçº³è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ240-è¥¿ç­ç‰™é©¬å¾·é‡Œè¯åˆ¸äº¤æ˜“æ‰€ï¼Œ260-çˆ±å°”å…°è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ280-è²å¾‹å®¾è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ310-æœºæ„é—´ç§å‹Ÿäº§å“æŠ¥ä»·ä¸æœåŠ¡ç³»ç»Ÿï¼Œ320-ä¿„ç½—æ–¯è«æ–¯ç§‘è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ390-é‡Œæ–¯æœ¬è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ400-èŠåŠ å“¥æœŸæƒäº¤æ˜“æ‰€ï¼Œ620-èƒ¡å¿—æ˜å¸‚è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ630-æ²ªå¸‚ä»£ç†æ·±å¸‚å¸‚åœºï¼Œ631-æ²ªå¸‚ä»£ç†æ¸¯äº¤æ‰€å¸‚åœºï¼Œ640-æ·±å¸‚ä»£ç†æ²ªå¸‚å¸‚åœºï¼Œ641-æ·±å¸‚ä»£ç†æ¸¯äº¤æ‰€å¸‚åœºï¼Œ650-å›½é™…å¤–æ±‡å¸‚åœº(æ™¨æ˜Ÿ)ï¼Œ653-ä¸Šæµ·ç¯å¢ƒèƒ½æºäº¤æ˜“æ‰€ï¼Œ654-åŒ—äº¬ç»¿è‰²äº¤æ˜“æ‰€ï¼Œ655-å¤©æ´¥ç¢³æ’æ”¾æƒäº¤æ˜“ä¸­å¿ƒï¼Œ657-æ¹–åŒ—ç¢³æ’æ”¾æƒäº¤æ˜“ä¸­å¿ƒï¼Œ658-é‡åº†ç¢³æ’æ”¾æƒäº¤æ˜“ä¸­å¿ƒï¼Œ659-å››å·è”åˆç¯å¢ƒäº¤æ˜“æ‰€ï¼Œ660-å¹¿å·ç¢³æ’æ”¾æƒäº¤æ˜“æ‰€ï¼Œ661-æµ·å³¡è‚¡æƒäº¤æ˜“ä¸­å¿ƒï¼Œ662-æ·±åœ³æ’æ”¾æƒäº¤æ˜“æ‰€ï¼Œ663-æ¬§æ´²èƒ½æºäº¤æ˜“æ‰€ï¼Œ664-å…¨å›½ç¢³æ’æ”¾æƒäº¤æ˜“ï¼Œ666-å¸ƒè¾¾ä½©æ–¯è¯åˆ¸äº¤æ˜“æ‰€ï¼Œ667-å…¨å›½æ¸©å®¤æ°”ä½“è‡ªæ„¿å‡æ’äº¤æ˜“å¸‚åœºï¼Œ66302-éŸ©å›½ETSï¼Œ66303-åŠ æ‹¿å¤§é­åŒ—å…‹Cap-and-Trade(CaT)ï¼Œ66305-ç¾å›½åŒºåŸŸæ¸©å®¤æ°”ä½“å€¡è®®ï¼ˆRGGIï¼‰ã€‚æ”¶èµ·'''
    # print(simplify_comment_with_llm(str))
